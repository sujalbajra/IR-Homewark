{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Data Preparation for Information Retrieval\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Theory: Document Collections](#theory)\n",
    "3. [Loading Documents](#loading)\n",
    "4. [Data Exploration](#exploration)\n",
    "5. [Summary](#summary)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "**Information Retrieval (IR)** is the process of obtaining relevant information from a large collection of documents based on a user's query. This is the foundation of search engines like Google, Bing, and specialized search systems.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Document**: A unit of text (article, webpage, book chapter, etc.)\n",
    "- **Collection/Corpus**: A set of documents\n",
    "- **Query**: User's information need expressed as text\n",
    "- **Relevance**: How well a document satisfies a query\n",
    "\n",
    "In this notebook, we'll prepare our Nepali document collection for IR experiments.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Theory: Document Collections <a name=\"theory\"></a>\n",
    "\n",
    "### What is a Document Collection?\n",
    "A document collection is a set of text documents that forms the basis for information retrieval. Standard IR collections include:\n",
    "\n",
    "- **TREC Collections**: Benchmark datasets for IR research\n",
    "- **Reuters Corpus**: News articles\n",
    "- **Cranfield Collection**: Historical IR test collection\n",
    "\n",
    "### Our Collection:\n",
    "We have 10 Nepali documents covering various topics:\n",
    "- History & Culture\n",
    "- Tourism & Geography\n",
    "- Education & Technology\n",
    "- Agriculture & Economy\n",
    "- Language & Literature\n",
    "- Sports & Entertainment\n",
    "- Healthcare\n",
    "- Transportation & Communication\n",
    "- Environment & Climate\n",
    "- Politics & Governance\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Loading Documents <a name=\"loading\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 10 documents\n",
      "âœ“ Document IDs: ['doc01', 'doc02', 'doc03', 'doc04', 'doc05', 'doc06', 'doc07', 'doc08', 'doc09', 'doc10']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define data directory path\n",
    "DATA_DIR = Path('../data')\n",
    "\n",
    "def load_documents(data_dir):\n",
    "    \"\"\"\n",
    "    Load all text documents from the data directory.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dir : Path\n",
    "        Path to the directory containing document files\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary mapping document IDs to document text\n",
    "    \"\"\"\n",
    "    documents = {}\n",
    "    \n",
    "    # Get all .txt files\n",
    "    for file_path in sorted(data_dir.glob('doc*.txt')):\n",
    "        doc_id = file_path.stem  # e.g., 'doc01'\n",
    "        \n",
    "        # Read document with UTF-8 encoding for Nepali text\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            documents[doc_id] = f.read()\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load all documents\n",
    "documents = load_documents(DATA_DIR)\n",
    "\n",
    "print(f\"âœ“ Loaded {len(documents)} documents\")\n",
    "print(f\"âœ“ Document IDs: {list(documents.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Data Exploration <a name=\"exploration\"></a>\n",
    "\n",
    "Let's explore our document collection to understand its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Document Statistics:\n",
      "======================================================================\n",
      "Doc ID     Lines    Words    Characters   Avg Word Len\n",
      "======================================================================\n",
      "doc01      8        87       568          6.53\n",
      "doc02      8        80       540          6.75\n",
      "doc03      8        72       542          7.53\n",
      "doc04      8        76       526          6.92\n",
      "doc05      8        77       547          7.1\n",
      "doc06      8        75       537          7.16\n",
      "doc07      8        71       517          7.28\n",
      "doc08      8        88       623          7.08\n",
      "doc09      8        86       569          6.62\n",
      "doc10      10       85       576          6.78\n",
      "======================================================================\n",
      "TOTAL               797      5545        \n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def get_document_stats(documents):\n",
    "    \"\"\"\n",
    "    Calculate statistics for each document in the collection.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    documents : dict\n",
    "        Dictionary of document ID to text\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Statistics for each document\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    for doc_id, text in documents.items():\n",
    "        # Basic statistics\n",
    "        lines = text.split('\\n')\n",
    "        words = text.split()\n",
    "        chars = len(text)\n",
    "        \n",
    "        stats[doc_id] = {\n",
    "            'lines': len(lines),\n",
    "            'words': len(words),\n",
    "            'characters': chars,\n",
    "            'avg_word_length': round(chars / len(words), 2) if words else 0\n",
    "        }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Get statistics\n",
    "stats = get_document_stats(documents)\n",
    "\n",
    "# Display statistics\n",
    "print(\"\\nðŸ“Š Document Statistics:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Doc ID':<10} {'Lines':<8} {'Words':<8} {'Characters':<12} {'Avg Word Len'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for doc_id, stat in stats.items():\n",
    "    print(f\"{doc_id:<10} {stat['lines']:<8} {stat['words']:<8} {stat['characters']:<12} {stat['avg_word_length']}\")\n",
    "\n",
    "# Overall statistics\n",
    "total_words = sum(s['words'] for s in stats.values())\n",
    "total_chars = sum(s['characters'] for s in stats.values())\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"{'TOTAL':<10} {'':<8} {total_words:<8} {total_chars:<12}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“„ Sample Document (doc01):\n",
      "======================================================================\n",
      "à¤¨à¥‡à¤ªà¤¾à¤²à¤•à¥‹ à¤‡à¤¤à¤¿à¤¹à¤¾à¤¸ à¤° à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿\n",
      "\n",
      "à¤¨à¥‡à¤ªà¤¾à¤² à¤¦à¤•à¥à¤·à¤¿à¤£ à¤à¤¶à¤¿à¤¯à¤¾à¤®à¤¾ à¤…à¤µà¤¸à¥à¤¥à¤¿à¤¤ à¤à¤‰à¤Ÿà¤¾ à¤¸à¥à¤¨à¥à¤¦à¤° à¤¹à¤¿à¤®à¤¾à¤²à¥€ à¤¦à¥‡à¤¶ à¤¹à¥‹à¥¤ à¤¯à¥‹ à¤¦à¥‡à¤¶ à¤†à¤«à¥à¤¨à¥‹ à¤¸à¤®à¥ƒà¤¦à¥à¤§ à¤‡à¤¤à¤¿à¤¹à¤¾à¤¸ à¤° à¤µà¤¿à¤µà¤¿à¤§ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿à¤•à¥‹ à¤²à¤¾à¤—à¤¿ à¤µà¤¿à¤¶à¥à¤µà¤­à¤° à¤ªà¥à¤°à¤¸à¤¿à¤¦à¥à¤§ à¤›à¥¤ à¤¨à¥‡à¤ªà¤¾à¤²à¤®à¤¾ à¤µà¤¿à¤­à¤¿à¤¨à¥à¤¨ à¤œà¤¾à¤¤à¤œà¤¾à¤¤à¤¿ à¤° à¤§à¤°à¥à¤®à¤•à¤¾ à¤®à¤¾à¤¨à¤¿à¤¸à¤¹à¤°à¥‚ à¤¸à¤¦à¥à¤­à¤¾à¤µà¤ªà¥‚à¤°à¥à¤µà¤• à¤¬à¤¸à¥‹à¤¬à¤¾à¤¸ à¤—à¤°à¥à¤›à¤¨à¥à¥¤\n",
      "\n",
      "à¤¨à¥‡à¤ªà¤¾à¤²à¤•à¥‹ à¤‡à¤¤à¤¿à¤¹à¤¾à¤¸ à¤¹à¤œà¤¾à¤°à¥Œà¤‚ à¤µà¤°à¥à¤· à¤ªà¥à¤°à¤¾à¤¨à¥‹ à¤›à¥¤ à¤•à¤¿à¤°à¤¾à¤à¤¤ à¤µà¤‚à¤¶, à¤²à¤¿à¤šà¥à¤›à¤µà¤¿ à¤µà¤‚à¤¶ à¤° à¤®à¤²à¥à¤² à¤µà¤‚à¤¶à¤²à¥‡ à¤¨...\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display first document as example\n",
    "print(\"\\nðŸ“„ Sample Document (doc01):\")\n",
    "print(\"=\"*70)\n",
    "print(documents['doc01'][:300] + \"...\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‘ Document Titles:\n",
      "======================================================================\n",
      "doc01: à¤¨à¥‡à¤ªà¤¾à¤²à¤•à¥‹ à¤‡à¤¤à¤¿à¤¹à¤¾à¤¸ à¤° à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿\n",
      "doc02: à¤¹à¤¿à¤®à¤¾à¤² à¤° à¤ªà¤°à¥à¤¯à¤Ÿà¤¨\n",
      "doc03: à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤° à¤ªà¥à¤°à¤µà¤¿à¤§à¤¿\n",
      "doc04: à¤•à¥ƒà¤·à¤¿ à¤° à¤…à¤°à¥à¤¥à¤¤à¤¨à¥à¤¤à¥à¤°\n",
      "doc05: à¤­à¤¾à¤·à¤¾ à¤° à¤¸à¤¾à¤¹à¤¿à¤¤à¥à¤¯\n",
      "doc06: à¤–à¥‡à¤²à¤•à¥à¤¦ à¤° à¤®à¤¨à¥‹à¤°à¤žà¥à¤œà¤¨\n",
      "doc07: à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤¸à¥‡à¤µà¤¾\n",
      "doc08: à¤¯à¤¾à¤¤à¤¾à¤¯à¤¾à¤¤ à¤° à¤¸à¤žà¥à¤šà¤¾à¤°\n",
      "doc09: à¤µà¤¾à¤¤à¤¾à¤µà¤°à¤£ à¤° à¤œà¤²à¤µà¤¾à¤¯à¥\n",
      "doc10: à¤°à¤¾à¤œà¤¨à¥€à¤¤à¤¿ à¤° à¤¶à¤¾à¤¸à¤¨ à¤µà¥à¤¯à¤µà¤¸à¥à¤¥à¤¾\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Extract titles from each document (first line)\n",
    "def extract_titles(documents):\n",
    "    \"\"\"\n",
    "    Extract the title (first line) from each document.\n",
    "    \"\"\"\n",
    "    titles = {}\n",
    "    for doc_id, text in documents.items():\n",
    "        first_line = text.split('\\n')[0].strip()\n",
    "        titles[doc_id] = first_line\n",
    "    return titles\n",
    "\n",
    "titles = extract_titles(documents)\n",
    "\n",
    "print(\"\\nðŸ“‘ Document Titles:\")\n",
    "print(\"=\"*70)\n",
    "for doc_id, title in titles.items():\n",
    "    print(f\"{doc_id}: {title}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Summary <a name=\"summary\"></a>\n",
    "\n",
    "### What We Learned:\n",
    "1. **Document Collections**: Organized sets of documents for IR experiments\n",
    "2. **Loading Data**: Read text files with proper encoding (UTF-8 for Nepali)\n",
    "3. **Basic Statistics**: Analyzed document characteristics (words, characters, lines)\n",
    "\n",
    "### Next Steps:\n",
    "In the next notebook (`02_text_preprocessing.ipynb`), we will:\n",
    "- Tokenize documents into words\n",
    "- Remove stopwords\n",
    "- Apply stemming using our custom mapping\n",
    "- Prepare documents for indexing\n",
    "\n",
    "### Key Takeaways:\n",
    "- IR systems work with **document collections**\n",
    "- Understanding your data is the **first step** in IR\n",
    "- Proper **encoding** is crucial for non-English languages\n",
    "- Document statistics help us understand collection characteristics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
