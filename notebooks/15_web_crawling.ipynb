{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 15. Web Crawling & Scraping Concepts\\n\n",
                "\\n\n",
                "A **Web Crawler** (spider) systematically browses the World Wide Web for indexing.\\n\n",
                "\\n\n",
                "## Key Components:\\n\n",
                "1. **Frontier**: List of URLs to visit.\\n\n",
                "2. **Fetcher**: Retrieves page content.\\n\n",
                "3. **Parser**: Extracts links and content.\\n\n",
                "4. **Politeness Policy**: Respect `robots.txt` and avoid overloading servers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from collections import deque\n",
                "import time\n",
                "import random\n",
                "import urllib.parse\n",
                "\n",
                "# Simulation Setup\\n\n",
                "# Since we can't crawl the real web, we simulate a mini-web.\\n\n",
                "WEB_GRAPH = {\n",
                "    \"http://nepal.gov.np\": [\"http://nepal.gov.np/news\", \"http://nepal.gov.np/contact\"],\n",
                "    \"http://nepal.gov.np/news\": [\"http://nepal.gov.np/news/article1\", \"http://news.com/nepal\"],\n",
                "    \"http://news.com/nepal\": [\"http://news.com\", \"http://nepal.gov.np\"],\n",
                "    \"http://news.com\": [\"http://news.com/world\", \"http://news.com/tech\"],\n",
                "    \"http://example.org\": [\"http://example.org/about\"],\n",
                "}\n",
                "\n",
                "def fetch_url(url):\n",
                "    # Simulate network delay\\n\n",
                "    time.sleep(0.1)\n",
                "    \n",
                "    if url in WEB_GRAPH:\n",
                "        return 200, WEB_GRAPH[url]\n",
                "    return 404, []"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. BFS Crawler (Breadth-First Search)\\n\n",
                "Standard crawling strategy: visit all neighbors before going deeper."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting BFS Crawl at: http://nepal.gov.np\n",
                        "  Fetching: http://nepal.gov.np ... OK\n",
                        "  Fetching: http://nepal.gov.np/news ... OK\n",
                        "  Fetching: http://nepal.gov.np/contact ... Failed (404)\n",
                        "  Fetching: http://nepal.gov.np/news/article1 ... Failed (404)\n",
                        "  Fetching: http://news.com/nepal ... OK\n",
                        "  Fetching: http://news.com ... OK\n",
                        "  Fetching: http://news.com/world ... Failed (404)\n",
                        "  Fetching: http://news.com/tech ... Failed (404)\n",
                        "Crawl complete. Visited 4 pages.\n"
                    ]
                }
            ],
            "source": [
                "def bfs_crawl(start_url, max_pages=10):\n",
                "    frontier = deque([start_url])\n",
                "    visited = set()\n",
                "    crawled_count = 0\n",
                "    \n",
                "    print(f\"Starting BFS Crawl at: {start_url}\")\n",
                "    \n",
                "    while frontier and crawled_count < max_pages:\n",
                "        url = frontier.popleft()\n",
                "        \n",
                "        if url in visited:\n",
                "            continue\n",
                "            \n",
                "        # Fetch\\n\n",
                "        print(f\"  Fetching: {url} ...\", end=\"\")\n",
                "        status, links = fetch_url(url)\n",
                "        \n",
                "        if status == 200:\n",
                "            print(\" OK\")\n",
                "            visited.add(url)\n",
                "            crawled_count += 1\n",
                "            \n",
                "            # Extract Links & Add to Frontier\\n\n",
                "            for link in links:\n",
                "                if link not in visited:\n",
                "                    frontier.append(link)\n",
                "        else:\n",
                "            print(\" Failed (404)\")\n",
                "            \n",
                "    print(f\"Crawl complete. Visited {len(visited)} pages.\")\n",
                "\n",
                "bfs_crawl(\"http://nepal.gov.np\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Politeness & Robots Exclusion\\n\n",
                "Real crawlers must respect `robots.txt`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Checking Robot Rules:\n",
                        "  http://nepal.gov.np/news -> Allowed\n",
                        "  http://nepal.gov.np/admin/login -> Blocked\n",
                        "  http://news.com/tech -> Allowed\n",
                        "  http://news.com/private/docs -> Blocked\n"
                    ]
                }
            ],
            "source": [
                "class RobotExclusion:\n",
                "    def __init__(self):\n",
                "        self.disallowed = {}\n",
                "        \n",
                "    def add_rule(self, domain, path):\n",
                "        if domain not in self.disallowed:\n",
                "            self.disallowed[domain] = []\n",
                "        self.disallowed[domain].append(path)\n",
                "        \n",
                "    def can_fetch(self, url):\n",
                "        parsed = urllib.parse.urlparse(url)\n",
                "        domain = f\"{parsed.scheme}://{parsed.netloc}\"\n",
                "        \n",
                "        if domain in self.disallowed:\n",
                "            for path in self.disallowed[domain]:\n",
                "                if parsed.path.startswith(path):\n",
                "                    return False\n",
                "        return True\n",
                "\n",
                "# Test Robot Rules\\n\n",
                "robots = RobotExclusion()\n",
                "robots.add_rule(\"http://nepal.gov.np\", \"/admin\")\n",
                "robots.add_rule(\"http://news.com\", \"/private\")\n",
                "\n",
                "test_urls = [\n",
                "    \"http://nepal.gov.np/news\",\n",
                "    \"http://nepal.gov.np/admin/login\",\n",
                "    \"http://news.com/tech\",\n",
                "    \"http://news.com/private/docs\"\n",
                "]\n",
                "\n",
                "print(\"\\nChecking Robot Rules:\")\n",
                "for u in test_urls:\n",
                "    allowed = robots.can_fetch(u)\n",
                "    status = \"Allowed\" if allowed else \"Blocked\"\n",
                "    print(f\"  {u} -> {status}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. URL Normalization\\n\n",
                "URLs must be canonicalized to avoid duplicates.\\n\n",
                "- `http://example.com` == `http://example.com/`\\n\n",
                "- `http://EXAMPLE.COM` == `http://example.com`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Normalization:\n",
                        "  HTTP://Nepal.Gov.NP -> http://nepal.gov.np/\n",
                        "  http://nepal.gov.np -> http://nepal.gov.np/\n",
                        "  http://nepal.gov.np/ -> http://nepal.gov.np/\n"
                    ]
                }
            ],
            "source": [
                "def normalize_url(url):\n",
                "    parsed = urllib.parse.urlparse(url)\n",
                "    scheme = parsed.scheme.lower()\n",
                "    netloc = parsed.netloc.lower()\n",
                "    path = parsed.path\n",
                "    \n",
                "    if not path:\n",
                "        path = \"/\"\n",
                "        \n",
                "    return f\"{scheme}://{netloc}{path}\"\n",
                "\n",
                "examples = [\n",
                "    \"HTTP://Nepal.Gov.NP\",\n",
                "    \"http://nepal.gov.np\",\n",
                "    \"http://nepal.gov.np/\"\n",
                "]\n",
                "\n",
                "print(\"\\nNormalization:\")\n",
                "for e in examples:\n",
                "    print(f\"  {e} -> {normalize_url(e)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
