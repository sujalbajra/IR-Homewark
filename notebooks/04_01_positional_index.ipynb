{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 04.01. Positional Index\n",
                "\n",
                "## Table of Contents\n",
                "1. [Introduction](#introduction)\n",
                "2. [Theory: Positional Indexing](#theory)\n",
                "3. [Building Positional Index](#building)\n",
                "4. [Phrase Queries](#phrases)\n",
                "5. [Proximity Queries](#proximity)\n",
                "6. [Summary](#summary)\n",
                "\n",
                "---\n",
                "\n",
                "## 1. Introduction <a name=\"introduction\"></a>\n",
                "\n",
                "**Positional Index** extends the inverted index by storing not just which documents contain a term, but also **where** in each document the term appears.\n",
                "\n",
                "### Why Positional Indexing?\n",
                "- **Phrase Queries**: Find \"‡§®‡•á‡§™‡§æ‡§≤ ‡§∏‡§∞‡§ï‡§æ‡§∞\" (exactly this phrase)\n",
                "- **Proximity Queries**: Find documents where two terms appear close together\n",
                "- **More Precise**: Better than simple Boolean AND\n",
                "\n",
                "### Example:\n",
                "```\n",
                "Document: \"‡§®‡•á‡§™‡§æ‡§≤ ‡§∏‡•Å‡§®‡•ç‡§¶‡§∞ ‡§¶‡•á‡§∂ ‡§π‡•ã‡•§ ‡§®‡•á‡§™‡§æ‡§≤ ‡§π‡§ø‡§Æ‡§æ‡§≤‡§ï‡•ã ‡§¶‡•á‡§∂ ‡§π‡•ã‡•§\"\n",
                "\n",
                "Simple Index:\n",
                "‡§®‡•á‡§™‡§æ‡§≤ ‚Üí {doc1}\n",
                "\n",
                "Positional Index:\n",
                "‡§®‡•á‡§™‡§æ‡§≤ ‚Üí {doc1: [0, 4]}  (appears at positions 0 and 4)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## 2. Theory: Positional Indexing <a name=\"theory\"></a>\n",
                "\n",
                "### Structure:\n",
                "```\n",
                "Term ‚Üí {DocID: [pos1, pos2, pos3, ...]}\n",
                "```\n",
                "\n",
                "### Storage Requirements:\n",
                "- **Simple Index**: `O(T √ó D)` where T = terms, D = docs\n",
                "- **Positional Index**: `O(T √ó D √ó P)` where P = avg positions per term\n",
                "- Typically **2-4x larger** than simple inverted index\n",
                "\n",
                "### Trade-off:\n",
                "- ‚úì More expressive queries\n",
                "- ‚úì Better precision\n",
                "- ‚úó Larger storage\n",
                "- ‚úó Slower to build\n",
                "\n",
                "---\n",
                "\n",
                "## 3. Building Positional Index <a name=\"building\"></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úì Loaded 10 documents\n"
                    ]
                }
            ],
            "source": [
                "from pathlib import Path\n",
                "from collections import defaultdict\n",
                "\n",
                "# Load data\n",
                "DATA_DIR = Path('../data')\n",
                "\n",
                "def load_documents(data_dir):\n",
                "    documents = {}\n",
                "    for file_path in sorted(data_dir.glob('doc*.txt')):\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            documents[file_path.stem] = f.read()\n",
                "    return documents\n",
                "\n",
                "def load_stopwords(file_path):\n",
                "    stopwords = set()\n",
                "    with open(file_path, 'r', encoding='utf-8') as f:\n",
                "        next(f)\n",
                "        for line in f:\n",
                "            stopwords.add(line.strip())\n",
                "    return stopwords\n",
                "\n",
                "def load_stemming_dict(file_path):\n",
                "    stem_dict = {}\n",
                "    with open(file_path, 'r', encoding='utf-8') as f:\n",
                "        next(f)\n",
                "        for line in f:\n",
                "            parts = line.strip().split(',')\n",
                "            if len(parts) == 2:\n",
                "                stem_dict[parts[0]] = parts[1]\n",
                "    return stem_dict\n",
                "\n",
                "def tokenize(text):\n",
                "    tokens = text.split()\n",
                "    cleaned = []\n",
                "    for token in tokens:\n",
                "        token = token.strip('‡•§,.!?;:\"\\'-()[]{}/')\n",
                "        if token and any('\\u0900' <= c <= '\\u097F' for c in token):\n",
                "            cleaned.append(token)\n",
                "    return cleaned\n",
                "\n",
                "def preprocess_text(text, stopwords, stem_dict):\n",
                "    tokens = tokenize(text)\n",
                "    tokens = [t for t in tokens if t not in stopwords]\n",
                "    tokens = [stem_dict.get(t, t) for t in tokens]\n",
                "    return tokens\n",
                "\n",
                "documents = load_documents(DATA_DIR)\n",
                "stopwords = load_stopwords(DATA_DIR / 'nepali_stopwords.csv')\n",
                "stem_dict = load_stemming_dict(DATA_DIR / 'nepali_stemming.csv')\n",
                "\n",
                "preprocessed_docs = {}\n",
                "for doc_id, text in documents.items():\n",
                "    preprocessed_docs[doc_id] = preprocess_text(text, stopwords, stem_dict)\n",
                "\n",
                "print(f\"‚úì Loaded {len(preprocessed_docs)} documents\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úì Built positional index\n",
                        "  Unique terms: 398\n",
                        "\n",
                        "üìå Example term: '‡§®‡•á‡§™‡§æ‡§≤'\n",
                        "   doc01: positions [0, 3, 19, 25, 36, 47]\n",
                        "   doc02: positions [2, 21, 23, 30]\n"
                    ]
                }
            ],
            "source": [
                "def build_positional_index(preprocessed_docs):\n",
                "    \"\"\"\n",
                "    Build positional index.\n",
                "    \n",
                "    Structure: {term: {doc_id: [pos1, pos2, ...]}}\n",
                "    \"\"\"\n",
                "    positional_index = defaultdict(lambda: defaultdict(list))\n",
                "    \n",
                "    for doc_id, terms in preprocessed_docs.items():\n",
                "        for position, term in enumerate(terms):\n",
                "            positional_index[term][doc_id].append(position)\n",
                "    \n",
                "    return dict(positional_index)\n",
                "\n",
                "# Build the index\n",
                "pos_index = build_positional_index(preprocessed_docs)\n",
                "\n",
                "print(f\"‚úì Built positional index\")\n",
                "print(f\"  Unique terms: {len(pos_index)}\")\n",
                "\n",
                "# Show example\n",
                "sample_term = list(pos_index.keys())[0]\n",
                "print(f\"\\nüìå Example term: '{sample_term}'\")\n",
                "for doc_id, positions in list(pos_index[sample_term].items())[:2]:\n",
                "    print(f\"   {doc_id}: positions {positions}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Phrase Queries <a name=\"phrases\"></a>\n",
                "\n",
                "A **phrase query** finds documents where terms appear consecutively.\n",
                "\n",
                "### Algorithm:\n",
                "1. Get posting lists for all terms in phrase\n",
                "2. Find documents containing ALL terms\n",
                "3. Check if positions are consecutive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîç Phrase Query: '‡§®‡•á‡§™‡§æ‡§≤ ‡§π‡§ø‡§Æ‡§æ‡§≤'\n",
                        "   Preprocessed: ['‡§®‡•á‡§™‡§æ‡§≤', '‡§π‡§ø‡§Æ‡§æ‡§≤']\n",
                        "\n",
                        "‚úì Documents containing phrase: {'doc02'}\n"
                    ]
                }
            ],
            "source": [
                "def phrase_query(phrase_terms, pos_index):\n",
                "    \"\"\"\n",
                "    Find documents containing the exact phrase.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    phrase_terms : list\n",
                "        Terms in the phrase (already preprocessed)\n",
                "    pos_index : dict\n",
                "        Positional index\n",
                "    \n",
                "    Returns:\n",
                "    --------\n",
                "    set : Document IDs containing the phrase\n",
                "    \"\"\"\n",
                "    if not phrase_terms:\n",
                "        return set()\n",
                "    \n",
                "    # Get documents containing first term\n",
                "    if phrase_terms[0] not in pos_index:\n",
                "        return set()\n",
                "    \n",
                "    candidate_docs = set(pos_index[phrase_terms[0]].keys())\n",
                "    \n",
                "    # Filter to docs containing all terms\n",
                "    for term in phrase_terms[1:]:\n",
                "        if term not in pos_index:\n",
                "            return set()\n",
                "        candidate_docs &= set(pos_index[term].keys())\n",
                "    \n",
                "    # Check for consecutive positions\n",
                "    result = set()\n",
                "    \n",
                "    for doc_id in candidate_docs:\n",
                "        # Get positions of first term\n",
                "        first_positions = pos_index[phrase_terms[0]][doc_id]\n",
                "        \n",
                "        for start_pos in first_positions:\n",
                "            # Check if subsequent terms appear at consecutive positions\n",
                "            found_phrase = True\n",
                "            \n",
                "            for i, term in enumerate(phrase_terms[1:], 1):\n",
                "                expected_pos = start_pos + i\n",
                "                if expected_pos not in pos_index[term][doc_id]:\n",
                "                    found_phrase = False\n",
                "                    break\n",
                "            \n",
                "            if found_phrase:\n",
                "                result.add(doc_id)\n",
                "                break  # Found phrase in this doc\n",
                "    \n",
                "    return result\n",
                "\n",
                "# Example phrase query\n",
                "phrase = \"‡§®‡•á‡§™‡§æ‡§≤ ‡§π‡§ø‡§Æ‡§æ‡§≤\"  # Replace with actual Nepali phrase\n",
                "phrase_tokens = preprocess_text(phrase, stopwords, stem_dict)\n",
                "\n",
                "print(f\"üîç Phrase Query: '{phrase}'\")\n",
                "print(f\"   Preprocessed: {phrase_tokens}\")\n",
                "\n",
                "results = phrase_query(phrase_tokens, pos_index)\n",
                "print(f\"\\n‚úì Documents containing phrase: {results}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Proximity Queries <a name=\"proximity\"></a>\n",
                "\n",
                "**Proximity queries** find terms within a certain distance of each other.\n",
                "\n",
                "Example: Find \"‡§®‡•á‡§™‡§æ‡§≤\" within 3 words of \"‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîç Proximity Query Example:\n",
                        "   (Would need actual Nepali terms in index)\n",
                        "\n",
                        "üí° Proximity queries allow flexible matching!\n"
                    ]
                }
            ],
            "source": [
                "def proximity_query(term1, term2, max_distance, pos_index):\n",
                "    \"\"\"\n",
                "    Find documents where term1 and term2 appear within max_distance.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    term1, term2 : str\n",
                "        Terms to search for\n",
                "    max_distance : int\n",
                "        Maximum distance between terms\n",
                "    pos_index : dict\n",
                "        Positional index\n",
                "    \n",
                "    Returns:\n",
                "    --------\n",
                "    dict : {doc_id: [(pos1, pos2), ...]}\n",
                "    \"\"\"\n",
                "    if term1 not in pos_index or term2 not in pos_index:\n",
                "        return {}\n",
                "    \n",
                "    # Find common documents\n",
                "    docs1 = set(pos_index[term1].keys())\n",
                "    docs2 = set(pos_index[term2].keys())\n",
                "    common_docs = docs1 & docs2\n",
                "    \n",
                "    result = {}\n",
                "    \n",
                "    for doc_id in common_docs:\n",
                "        positions1 = pos_index[term1][doc_id]\n",
                "        positions2 = pos_index[term2][doc_id]\n",
                "        \n",
                "        matches = []\n",
                "        for p1 in positions1:\n",
                "            for p2 in positions2:\n",
                "                if abs(p1 - p2) <= max_distance:\n",
                "                    matches.append((p1, p2))\n",
                "        \n",
                "        if matches:\n",
                "            result[doc_id] = matches\n",
                "    \n",
                "    return result\n",
                "\n",
                "# Example proximity query\n",
                "print(\"üîç Proximity Query Example:\")\n",
                "print(\"   (Would need actual Nepali terms in index)\")\n",
                "print(\"\\nüí° Proximity queries allow flexible matching!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Summary <a name=\"summary\"></a>\n",
                "\n",
                "### What We Learned:\n",
                "\n",
                "1. **Positional Index**\n",
                "   - Stores term positions within documents\n",
                "   - Structure: `{term: {doc: [positions]}}`\n",
                "   - 2-4x larger than simple index\n",
                "\n",
                "2. **Phrase Queries**\n",
                "   - Find exact phrase matches\n",
                "   - Check consecutive positions\n",
                "   - More precise than Boolean AND\n",
                "\n",
                "3. **Proximity Queries**\n",
                "   - Find terms within distance threshold\n",
                "   - Flexible matching\n",
                "   - Useful for related concepts\n",
                "\n",
                "### Comparison:\n",
                "\n",
                "| Query Type | Example | Matches |\n",
                "|------------|---------|----------|\n",
                "| Boolean AND | ‡§®‡•á‡§™‡§æ‡§≤ AND ‡§π‡§ø‡§Æ‡§æ‡§≤ | Both terms anywhere |\n",
                "| Phrase | \"‡§®‡•á‡§™‡§æ‡§≤ ‡§π‡§ø‡§Æ‡§æ‡§≤\" | Exact consecutive |\n",
                "| Proximity | ‡§®‡•á‡§™‡§æ‡§≤ /3 ‡§π‡§ø‡§Æ‡§æ‡§≤ | Within 3 words |\n",
                "\n",
                "### Limitations:\n",
                "- Larger storage requirements\n",
                "- Slower query processing\n",
                "- Not suitable for very large corpora without optimization\n",
                "\n",
                "### Extensions:\n",
                "- **Bi-word indexes**: Store pairs of consecutive words\n",
                "- **Skip pointers**: Speed up proximity checks\n",
                "- **Compressed positions**: Reduce storage using deltas\n",
                "\n",
                "### References:\n",
                "- Manning et al., \"Introduction to Information Retrieval\", Chapter 2.4\n",
                "- Zobel & Moffat (2006): \"Inverted files for text search engines\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
