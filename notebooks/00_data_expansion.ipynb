{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 00. Data Expansion & Synthetic Generation\\n\n",
                "\\n\n",
                "To support advanced IR topics like **Topic Modeling (LDA)**, **Neural IR**, and **Evaluation**, we need more data than our initial 10 documents.\\n\n",
                "\\n\n",
                "This notebook generates:\\n\n",
                "1. **Synthetic Corpus**: 50+ documents across 5 topics.\\n\n",
                "2. **Evaluation Dataset**: `relevance_judgments.json` (Queries + Relevant Docs).\\n\n",
                "3. **Dummy Embeddings**: `word_vectors.json` (Simulated Word2Vec for Neural IR demo).\\n\n",
                "\\n\n",
                "**Note:** We use a simple template-based generator to create grammatically \"okay\" Nepali sentences for algorithmic testing. Content meaning may be nonsensical."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import random\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "\n",
                "DATA_DIR = Path('../data')\n",
                "DATA_DIR.mkdir(exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Synthetic Corpus Generation\\n\n",
                "\\n\n",
                "We define 5 topics with specific vocabularies. We generate documents by randomly combining these words into sentence templates."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating documents...\n",
                        "✓ Generated 50 new documents.\n"
                    ]
                }
            ],
            "source": [
                "topics = {\n",
                "    \"politics\": {\n",
                "        \"keywords\": [\"सरकार\", \"मन्त्री\", \"चुनाव\", \"संसद\", \"संविधान\", \"राजनीति\", \"नेता\", \"जनता\", \"लोकतन्त्र\", \"अधिकार\"],\n",
                "        \"sentences\": [\n",
                "            \"सरकारले जनताको अधिकार सुनिश्चित गर्नुपर्छ।\",\n",
                "            \"नयाँ संविधान जारी भएपछि चुनाव भयो।\",\n",
                "            \"संसदमा मन्त्रीले भाषण गरे।\",\n",
                "            \"राजनीति देशको मेरुदण्ड हो।\"\n",
                "        ]\n",
                "    },\n",
                "    \"sports\": {\n",
                "        \"keywords\": [\"फुटबल\", \"क्रिकेट\", \"खेलकुद\", \"मैदान\", \"खेलाडी\", \"प्रतियोगिता\", \"गोल\", \"ब्याटिङ\", \"जित\", \"हार\"],\n",
                "        \"sentences\": [\n",
                "            \"नेपालले क्रिकेट प्रतियोगिता जित्यो।\",\n",
                "            \"खेलाडीले मैदानमा अभ्यास गरे।\",\n",
                "            \"फुटबल खेल रोमाञ्चक थियो।\",\n",
                "            \"खेलकुदले स्वास्थ्यलाई फाइदा गर्छ।\"\n",
                "        ]\n",
                "    },\n",
                "    \"technology\": {\n",
                "        \"keywords\": [\"कम्प्युटर\", \"इन्टरनेट\", \"मोबाइल\", \"सफ्टवेयर\", \"डिजिटल\", \"प्रविधि\", \"डाटा\", \"अनलाइन\", \"वेबसाइट\", \"एप\"],\n",
                "        \"sentences\": [\n",
                "            \"आजकल सबै काम इन्टरनेटबाट हुन्छ।\",\n",
                "            \"मैले नयाँ मोबाइल किने।\",\n",
                "            \"कम्प्युटर प्रविधिले विकास ल्याएको छ।\",\n",
                "            \"सफ्टवेयर इन्जिनियरहरू कोड लेख्छन्।\"\n",
                "        ]\n",
                "    },\n",
                "    \"travel\": {\n",
                "        \"keywords\": [\"हिमाल\", \"पर्यटन\", \"पदयात्रा\", \"होटल\", \"पोखरा\", \"सगरमाथा\", \"पर्यटक\", \"यात्रा\", \"प्राकृतिक\", \"दृश्य\"],\n",
                "        \"sentences\": [\n",
                "            \"नेपाल हिमालको देश हो।\",\n",
                "            \"पोखरामा धेरै पर्यटक आउँछन्।\",\n",
                "            \"सगरमाथा चढ्न विदेशीहरू आउँछन्।\",\n",
                "            \"हामी पदयात्रामा गयौं।\"\n",
                "        ]\n",
                "    },\n",
                "    \"culture\": {\n",
                "        \"keywords\": [\"चाडपर्व\", \"संस्कृति\", \"दशैं\", \"तिहार\", \"मन्दिर\", \"पूजा\", \"परम्परा\", \"धर्म\", \"जात्रा\", \"भेषभुषा\"],\n",
                "        \"sentences\": [\n",
                "            \"नेपालमा धेरै चाडपर्व मनाइन्छ।\",\n",
                "            \"दशैं नेपालीहरूको महान चाड हो।\",\n",
                "            \"हामी मन्दिरमा पूजा गर्छौं।\",\n",
                "            \"हाम्रो संस्कृति धेरै धनी छ।\"\n",
                "        ]\n",
                "    }\n",
                "}\n",
                "\n",
                "def generate_document(topic_name, doc_id):\n",
                "    data = topics[topic_name]\n",
                "    # Base content\n",
                "    content = [random.choice(data[\"sentences\"]) for _ in range(3)]\n",
                "    \n",
                "    # Add random keywords for density\n",
                "    extra_keywords = random.sample(data[\"keywords\"], 5)\n",
                "    content.append(\" \".join(extra_keywords) + \"।\")\n",
                "    \n",
                "    # Mix topic just a little bit (noise) - 10% chance\n",
                "    if random.random() < 0.1:\n",
                "        other_topic = random.choice(list(topics.keys()))\n",
                "        content.append(random.choice(topics[other_topic][\"sentences\"]))\n",
                "        \n",
                "    text = \" \" .join(content)\n",
                "    \n",
                "    filename = DATA_DIR / f\"doc{doc_id:03d}_{topic_name}.txt\"\n",
                "    with open(filename, 'w', encoding='utf-8') as f:\n",
                "        f.write(text)\n",
                "    return filename\n",
                "\n",
                "# Generate 50 documents (10 per topic)\n",
                "doc_count = 10  # Start from 11 (assuming 1-10 exist)\n",
                "generated_files = []\n",
                "\n",
                "print(\"Generating documents...\")\n",
                "for topic in topics:\n",
                "    for _ in range(10):\n",
                "        doc_count += 1\n",
                "        filepath = generate_document(topic, doc_count)\n",
                "        generated_files.append(filepath.name)\n",
                "        \n",
                "print(f\"✓ Generated {len(generated_files)} new documents.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Evaluation Ground Truth\\n\n",
                "\\n\n",
                "For evaluating our search engine (NDCG, MAP), we need \"correct answers\". We will create a JSON file mapping queries to relevant document IDs based on the topics we just generated."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ Generated relevance_judgments.json\n"
                    ]
                }
            ],
            "source": [
                "relevance_data = {\n",
                "    \"queries\": {\n",
                "        \"q1\": \"नेपालको राजनीति र सरकार\",\n",
                "        \"q2\": \"फुटबल र क्रिकेट खेल\",\n",
                "        \"q3\": \"कम्प्युटर प्रविधि\",\n",
                "        \"q4\": \"हिमाल आरोहण पर्यटन\",\n",
                "        \"q5\": \"नेपाली संस्कृति र चाडपर्व\"\n",
                "    },\n",
                "    \"assessments\": {}\n",
                "}\n",
                "\n",
                "# Auto-generate assessments based on filenames\n",
                "# Files are named like doc011_politics.txt\n",
                "all_files = sorted([f.name for f in DATA_DIR.glob(\"*.txt\")])\n",
                "\n",
                "topic_map = {\n",
                "    \"q1\": \"politics\",\n",
                "    \"q2\": \"sports\",\n",
                "    \"q3\": \"technology\",\n",
                "    \"q4\": \"travel\",\n",
                "    \"q5\": \"culture\"\n",
                "}\n",
                "\n",
                "for q_id, topic in topic_map.items():\n",
                "    relevance_data[\"assessments\"][q_id] = {}\n",
                "    \n",
                "    for fname in all_files:\n",
                "        doc_id = fname.split('.')[0] # doc011_politics\n",
                "        \n",
                "        score = 0\n",
                "        if topic in fname: # Highly relevant\n",
                "            score = 3\n",
                "        elif random.random() < 0.05: # Random noise/partial relevance\n",
                "            score = 1\n",
                "            \n",
                "        if score > 0:\n",
                "            relevance_data[\"assessments\"][q_id][doc_id] = score\n",
                "\n",
                "with open(DATA_DIR / 'relevance_judgments.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(relevance_data, f, indent=4, ensure_ascii=False)\n",
                "    \n",
                "print(\"✓ Generated relevance_judgments.json\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Dummy Vectors for Neural IR\\n\n",
                "\\n\n",
                "Training a Word2Vec model on 60 small documents is useless. For the Neural IR notebook, we will generate **dummy pre-trained vectors**.\\n\n",
                "\\n\n",
                "We will enforce semantic relationships manually:\\n\n",
                "- 'politics' words will be close to each other in vector space.\\n\n",
                "- 'sports' words will be far from 'politics'."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ Generated word_vectors.json with 50 words.\n",
                        "  Note: 'सरकार' and 'मन्त्री' should be mathematically close now.\n"
                    ]
                }
            ],
            "source": [
                "def create_dummy_vectors(topics, dim=50):\n",
                "    vocab_vectors = {}\n",
                "    \n",
                "    # Generate a base center vector for each topic\n",
                "    topic_centers = {t: np.random.rand(dim) for t in topics}\n",
                "    \n",
                "    all_words = []\n",
                "    for t in topics.values():\n",
                "        all_words.extend(t[\"keywords\"])\n",
                "    \n",
                "    for topic_name, data in topics.items():\n",
                "        center = topic_centers[topic_name]\n",
                "        \n",
                "        for word in data[\"keywords\"]:\n",
                "            # Word vector = Topic Center + Small Random Noise\n",
                "            noise = np.random.normal(0, 0.1, dim)\n",
                "            vector = center + noise\n",
                "            vocab_vectors[word] = vector.tolist()\n",
                "            \n",
                "    return vocab_vectors\n",
                "\n",
                "vectors = create_dummy_vectors(topics)\n",
                "\n",
                "with open(DATA_DIR / 'word_vectors.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(vectors, f, indent=4, ensure_ascii=False)\n",
                "    \n",
                "print(f\"✓ Generated word_vectors.json with {len(vectors)} words.\")\n",
                "print(\"  Note: 'सरकार' and 'मन्त्री' should be mathematically close now.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
