{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 13. Topic Modeling (LDA) from Scratch\\n\n",
                "\\n\n",
                "Topic modeling is an unsupervised machine learning technique to discover abstract \"topics\" that occur in a collection of documents.\\n\n",
                "\\n\n",
                "We will implement a simplified version of **Latent Dirichlet Allocation (LDA)** using **Gibbs Sampling** concepts in vanilla Python.\\n\n",
                "\\n\n",
                "## Goal:\\n\n",
                "Given our synthetic 50+ documents, can we automatically rediscover the 5 topics (Politics, Sports, Technology, Travel, Culture)?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 60 documents.\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import glob\n",
                "import random\n",
                "import math\n",
                "from collections import Counter, defaultdict\n",
                "\n",
                "DATA_DIR = \"../data\"\n",
                "\n",
                "def load_documents(data_dir):\n",
                "    documents = []\n",
                "    filenames = sorted(glob.glob(os.path.join(data_dir, \"doc*.txt\")))\n",
                "    for filepath in filenames:\n",
                "        with open(filepath, 'r', encoding='utf-8') as f:\n",
                "            documents.append(f.read())\n",
                "    return documents, filenames\n",
                "\n",
                "raw_documents, filenames = load_documents(DATA_DIR)\n",
                "print(f\"Loaded {len(raw_documents)} documents.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Preprocessing\\n\n",
                "We need to tokenize and remove stopwords to leave only content words."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Vocabulary size: 516\n"
                    ]
                }
            ],
            "source": [
                "STOPWORDS = {\"र\", \"को\", \"मा\", \"हामी\", \"यो\", \"लागि\", \"ले\", \"का\", \"हरु\", \"तथा\", \"भने\", \"भयो\", \"छ\", \"हो\", \"पनि\", \"गर्न\", \"गरे\"}\n",
                "\n",
                "def preprocess(text):\n",
                "    tokens = text.split()\n",
                "    clean = []\n",
                "    for t in tokens:\n",
                "        # Remove punctuation\n",
                "        t = t.strip('।,.!?;:\"\\'-()[]{}/')\n",
                "        if t and t not in STOPWORDS:\n",
                "            clean.append(t)\n",
                "    return clean\n",
                "\n",
                "docs = [preprocess(d) for d in raw_documents]\n",
                "\n",
                "# Build Vocabulary\n",
                "vocab = set(word for doc in docs for word in doc)\n",
                "w2id = {w: i for i, w in enumerate(vocab)}\n",
                "id2w = {i: w for w, i in w2id.items()}\n",
                "\n",
                "print(f\"Vocabulary size: {len(vocab)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Simple LDA Implementation (Gibbs Sampling)\\n\n",
                "\\n\n",
                "**Core Idea**: \\n\n",
                "1. Randomly assign topics to each word in each document.\\n\n",
                "2. Iteratively update topic assignment for each word based on:\\n\n",
                "   - How prevalent is that topic in this document? $P(z|d)$\\n\n",
                "   - How prevalent is this word in that topic? $P(w|z)$\\n\n",
                "\\n\n",
                "$$ P(z_i = k | \\dots) \\propto (n_{d,k} + \\alpha) \\times \\frac{n_{k,w} + \\beta}{n_k + V\\beta} $$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleLDA:\n",
                "    def __init__(self, K, alpha=0.1, beta=0.1, iterations=20):\n",
                "        self.K = K  # Number of topics\n",
                "        self.alpha = alpha\n",
                "        self.beta = beta\n",
                "        self.iterations = iterations\n",
                "\n",
                "    def fit(self, docs, vocab_size):\n",
                "        self.docs = docs\n",
                "        self.V = vocab_size\n",
                "        \n",
                "        # Initialize counts\n",
                "        self.nd_k = defaultdict(lambda: [0]*self.K) # Doc-Topic counts\n",
                "        self.nk_w = defaultdict(lambda: [0]*self.V) # Topic-Word counts\n",
                "        self.nk = [0]*self.K                        # Total words in topic k\n",
                "        \n",
                "        # Random assignment\n",
                "        self.z = [] # Topic assignments for every word in every doc\n",
                "        for d, doc in enumerate(docs):\n",
                "            z_doc = []\n",
                "            for w_id in doc:\n",
                "                topic = random.randint(0, self.K-1)\n",
                "                z_doc.append(topic)\n",
                "                \n",
                "                self.nd_k[d][topic] += 1\n",
                "                self.nk_w[topic][w_id] += 1\n",
                "                self.nk[topic] += 1\n",
                "            self.z.append(z_doc)\n",
                "            \n",
                "        # Gibbs Sampling\n",
                "        print(f\"Starting Gibbs Sampling ({self.iterations} iterations)...\")\n",
                "        for it in range(self.iterations):\n",
                "            for d, doc in enumerate(docs):\n",
                "                for i, w_id in enumerate(doc):\n",
                "                    # 1. Remove current assignment\n",
                "                    curr_topic = self.z[d][i]\n",
                "                    self.nd_k[d][curr_topic] -= 1\n",
                "                    self.nk_w[curr_topic][w_id] -= 1\n",
                "                    self.nk[curr_topic] -= 1\n",
                "                    \n",
                "                    # 2. Calculate probabilities for new assignment\n",
                "                    p_z = []\n",
                "                    for k in range(self.K):\n",
                "                        # P(topic | doc)\n",
                "                        p_doc = self.nd_k[d][k] + self.alpha\n",
                "                        # P(word | topic)\n",
                "                        p_word = (self.nk_w[k][w_id] + self.beta) / (self.nk[k] + self.V * self.beta)\n",
                "                        \n",
                "                        p_z.append(p_doc * p_word)\n",
                "                    \n",
                "                    # Normalize\n",
                "                    total_p = sum(p_z)\n",
                "                    probs = [p/total_p for p in p_z]\n",
                "                    \n",
                "                    # 3. Sample new topic (roulette wheel)\n",
                "                    r = random.random()\n",
                "                    acc = 0\n",
                "                    new_topic = 0\n",
                "                    for k, p in enumerate(probs):\n",
                "                        acc += p\n",
                "                        if r < acc:\n",
                "                            new_topic = k\n",
                "                            break\n",
                "                            \n",
                "                    # 4. updates\n",
                "                    self.z[d][i] = new_topic\n",
                "                    self.nd_k[d][new_topic] += 1\n",
                "                    self.nk_w[new_topic][w_id] += 1\n",
                "                    self.nk[new_topic] += 1\n",
                "            \n",
                "            if (it+1) % 5 == 0:\n",
                "                print(f\"  Iteration {it+1}/{self.iterations} complete\")\n",
                "\n",
                "    def get_topics(self, top_n=5):\n",
                "        topics = []\n",
                "        for k in range(self.K):\n",
                "            # Find top words for topic k\n",
                "            word_counts = []\n",
                "            for w_id in range(self.V):\n",
                "                count = self.nk_w[k][w_id]\n",
                "                word_counts.append((w_id, count))\n",
                "            \n",
                "            word_counts.sort(key=lambda x: x[1], reverse=True)\n",
                "            top_word_ids = word_counts[:top_n]\n",
                "            \n",
                "            topics.append([id2w[wid] for wid, _ in top_word_ids])\n",
                "        return topics"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train and Visualize\\n\n",
                "\\n\n",
                "We know we have 5 genres, so we set K=5."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting Gibbs Sampling (30 iterations)...\n",
                        "  Iteration 5/30 complete\n",
                        "  Iteration 10/30 complete\n",
                        "  Iteration 15/30 complete\n",
                        "  Iteration 20/30 complete\n",
                        "  Iteration 25/30 complete\n",
                        "  Iteration 30/30 complete\n",
                        "\n",
                        "Discovered Topics:\n",
                        "==============================\n",
                        "Topic 1: संविधान, चुनाव, राजनीति, अधिकार, संसदमा, मन्त्रीले, मन्त्री\n",
                        "Topic 2: क्रिकेट, फुटबल, प्रतियोगिता, अभ्यास, मैदानमा, खेलाडीले, खेलाडी\n",
                        "Topic 3: नेपाल, सगरमाथा, आउँछन्, देश, हिमालको, पर्यटक, चढ्न\n",
                        "Topic 4: संस्कृति, धेरै, मोबाइल, दशैं, सफ्टवेयर, चाडपर्व, लेख्छन्\n",
                        "Topic 5: नेपालमा, नेपालको, छन्, हुन्, नेपाली, जस्ता, विश्वविद्यालय\n"
                    ]
                }
            ],
            "source": [
                "# Convert docs to IDs\n",
                "doc_ids = []\n",
                "for doc in docs:\n",
                "    doc_ids.append([w2id[w] for w in doc])\n",
                "\n",
                "lda = SimpleLDA(K=5, iterations=30)\n",
                "lda.fit(doc_ids, len(vocab))\n",
                "\n",
                "print(\"\\nDiscovered Topics:\")\n",
                "print(\"=\"*30)\n",
                "topics = lda.get_topics(top_n=7)\n",
                "for i, t in enumerate(topics):\n",
                "    print(f\"Topic {i+1}: {', '.join(t)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Evaluation\\n\n",
                "Check if documents match their dominant topic."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Document Topic Distribution (Sample):\n",
                        "doc02.txt -> Dominant Topic 3 (0.71)\n",
                        "doc020_politics.txt -> Dominant Topic 1 (1.00)\n",
                        "doc021_sports.txt -> Dominant Topic 2 (1.00)\n",
                        "doc022_sports.txt -> Dominant Topic 2 (1.00)\n",
                        "doc023_sports.txt -> Dominant Topic 2 (1.00)\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\nDocument Topic Distribution (Sample):\")\n",
                "for i in range(10, 15): # Check some synthetic docs\n",
                "    doc_name = filenames[i]\n",
                "    counts = lda.nd_k[i]\n",
                "    total = sum(counts)\n",
                "    if total > 0:\n",
                "        props = [c/total for c in counts]\n",
                "        dominant = props.index(max(props))\n",
                "        print(f\"{os.path.basename(doc_name)} -> Dominant Topic {dominant+1} ({max(props):.2f})\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
