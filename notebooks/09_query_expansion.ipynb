{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 09. Enhanced Query Expansion\\n\n",
                "\\n\n",
                "## Introduction\\n\n",
                "\\n\n",
                "Query Expansion aims to solve vocabulary mismatch by adding related terms to the user's query. In this enhanced notebook, we implement:\\n\n",
                "\\n\n",
                "1. **Pseudo-Relevance Feedback**: Extracting terms from top-ranked documents.\\n\n",
                "2. **Global Analysis (Co-occurrence Matrix)**: Identifying terms that frequently appear together across the entire collection to find semantic associations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 10 documents\n"
                    ]
                }
            ],
            "source": [
                "from pathlib import Path\n",
                "from collections import Counter, defaultdict\n",
                "import math\n",
                "\n",
                "DATA_DIR = Path('../data')\n",
                "\n",
                "def load_documents(data_dir):\n",
                "    documents = {}\n",
                "    for file_path in sorted(data_dir.glob('doc*.txt')):\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            documents[file_path.stem] = f.read()\n",
                "    return documents\n",
                "\n",
                "def tokenize(text):\n",
                "    # Simple tokenization for demonstration\n",
                "    tokens = text.split()\n",
                "    cleaned = []\n",
                "    for token in tokens:\n",
                "        token = token.strip('।,.!?;:\"\\'-()[]{}/')\n",
                "        if token:\n",
                "            cleaned.append(token)\n",
                "    return cleaned\n",
                "\n",
                "documents = load_documents(DATA_DIR)\n",
                "print(f\"Loaded {len(documents)} documents\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Co-occurrence Matrix Construction\\n\n",
                "We build a term-term correlation matrix. Two terms are related if they co-occur in the same document (or window) frequently.\\n\n",
                "\\n\n",
                "Association Measure: **Mutual Information** or simple **Co-occurrence Count**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Built matrix with vocabulary size: 452\n"
                    ]
                }
            ],
            "source": [
                "class CooccurrenceMatrix:\n",
                "    def __init__(self, documents, window_size=5):\n",
                "        self.matrix = defaultdict(Counter)\n",
                "        self.vocab = set()\n",
                "        self.window_size = window_size\n",
                "        self._build(documents)\n",
                "        \n",
                "    def _build(self, documents):\n",
                "        for doc_text in documents.values():\n",
                "            tokens = tokenize(doc_text)\n",
                "            self.vocab.update(tokens)\n",
                "            \n",
                "            # Window-based co-occurrence\n",
                "            for i, target_term in enumerate(tokens):\n",
                "                start = max(0, i - self.window_size)\n",
                "                end = min(len(tokens), i + self.window_size + 1)\n",
                "                \n",
                "                context = tokens[start:i] + tokens[i+1:end]\n",
                "                for ctx_term in context:\n",
                "                    self.matrix[target_term][ctx_term] += 1\n",
                "                    \n",
                "    def get_related_terms(self, term, top_k=5):\n",
                "        if term not in self.matrix:\n",
                "            return []\n",
                "        \n",
                "        # Return most frequent co-occurring terms\n",
                "        return self.matrix[term].most_common(top_k)\n",
                "\n",
                "co_matrix = CooccurrenceMatrix(documents)\n",
                "print(f\"Built matrix with vocabulary size: {len(co_matrix.vocab)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Query Expansion using Co-occurrence\\n\n",
                "For each query term, we find highly correlated terms from the matrix and add them to the query."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Original terms: ['इतिहास']\n",
                        "  Related to 'इतिहास': [('नेपालको', 2), ('र', 2)]\n",
                        "\n",
                        "Expanded Query: इतिहास नेपालको र\n"
                    ]
                }
            ],
            "source": [
                "def expand_query_global(query, co_matrix, num_expansion_terms=2):\n",
                "    query_terms = tokenize(query)\n",
                "    expanded_query = list(query_terms)\n",
                "    \n",
                "    print(f\"Original terms: {query_terms}\")\n",
                "    \n",
                "    for term in query_terms:\n",
                "        related = co_matrix.get_related_terms(term, top_k=num_expansion_terms)\n",
                "        if related:\n",
                "            print(f\"  Related to '{term}': {related}\")\n",
                "            new_terms = [t for t, count in related]\n",
                "            expanded_query.extend(new_terms)\n",
                "            \n",
                "    # Remove duplicates while preserving order\n",
                "    final_query = list(dict.fromkeys(expanded_query))\n",
                "    return \" \".join(final_query)\n",
                "\n",
                "# Test\n",
                "test_query = \"इतिहास\"\n",
                "expanded = expand_query_global(test_query, co_matrix)\n",
                "print(f\"\\nExpanded Query: {expanded}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Pseudo-Relevance Feedback (Local Analysis)\\n\n",
                "Already discussed in placeholder, but here is a refined implementation assuming we have a ranking function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Feedback terms from ['doc01', 'doc03']: ['र', 'छ', 'नेपालको']\n"
                    ]
                }
            ],
            "source": [
                "# Placeholder ranking results for demonstration\n",
                "# In a real scenario, we would run `bm25.search(query)` first\n",
                "top_ranked_docs = ['doc01', 'doc03'] \n",
                "\n",
                "def extract_feedback_terms(relevant_doc_ids, documents, top_k=3):\n",
                "    term_counts = Counter()\n",
                "    for doc_id in relevant_doc_ids:\n",
                "        if doc_id in documents:\n",
                "            tokens = tokenize(documents[doc_id])\n",
                "            term_counts.update(tokens)\n",
                "            \n",
                "    return [term for term, count in term_counts.most_common(top_k)]\n",
                "\n",
                "feedback_terms = extract_feedback_terms(top_ranked_docs, documents)\n",
                "print(f\"Feedback terms from {top_ranked_docs}: {feedback_terms}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\\n\n",
                "Global analysis (Co-occurrence) is static and corpus-dependent, offering generic synonyms. Local analysis (Pseudo-Relevance Feedback) is dynamic and query-dependent, offering context-specific terms. Combining both yields the best results."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
