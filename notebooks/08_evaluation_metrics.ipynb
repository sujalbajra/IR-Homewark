{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 08. Evaluation Metrics for IR\\n\n",
                "\\n\n",
                "## Table of Contents\\n\n",
                "1. [Introduction](#introduction)\\n\n",
                "2. [Precision, Recall, F1](#basic-metrics)\\n\n",
                "3. [Mean Average Precision (MAP)](#map)\\n\n",
                "4. [Mean Reciprocal Rank (MRR)](#mrr)\\n\n",
                "5. [Normalized Discounted Cumulative Gain (NDCG)](#ndcg)\\n\n",
                "\\n\n",
                "---\\n\n",
                "\\n\n",
                "## 1. Introduction <a name=\"introduction\"></a>\\n\n",
                "Evaluating Information Retrieval systems is crucial to measure their effectiveness. We go beyond basic precision/recall to measuring ranked list quality."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "\n",
                "def calculate_precision_recall(retrieved, relevant):\n",
                "    if not retrieved:\n",
                "        return 0.0, 0.0\n",
                "    intersection = len(retrieved & relevant)\n",
                "    precision = intersection / len(retrieved)\n",
                "    recall = intersection / len(relevant) if relevant else 0.0\n",
                "    return precision, recall\n",
                "\n",
                "def calculate_f1(precision, recall):\n",
                "    if precision + recall == 0:\n",
                "        return 0.0\n",
                "    return 2 * (precision * recall) / (precision + recall)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Mean Average Precision (MAP) <a name=\"map\"></a>\\n\n",
                "MAP provides a single-figure measure of quality across recall levels. Among evaluation measures, MAP has been shown to have especially good discrimination and stability."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def average_precision(ranked_list, relevant_docs):\n",
                "    pk_sum = 0.0\n",
                "    num_rel = 0\n",
                "    for i, doc_id in enumerate(ranked_list):\n",
                "        if doc_id in relevant_docs:\n",
                "            num_rel += 1\n",
                "            pk = num_rel / (i + 1)\n",
                "            pk_sum += pk\n",
                "    if not relevant_docs:\n",
                "        return 0.0\n",
                "    return pk_sum / len(relevant_docs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Mean Reciprocal Rank (MRR) <a name=\"mrr\"></a>\\n\n",
                "\\n\n",
                "MRR is a statistic measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer.\\n\n",
                "\\n\n",
                "$$ MRR = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{1}{\\text{rank}_i} $$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MRR Score: 0.6667\n"
                    ]
                }
            ],
            "source": [
                "def reciprocal_rank(ranked_list, relevant_docs):\n",
                "    for i, doc_id in enumerate(ranked_list):\n",
                "        if doc_id in relevant_docs:\n",
                "            return 1.0 / (i + 1)\n",
                "    return 0.0\n",
                "\n",
                "def mean_reciprocal_rank(query_results, query_relevance):\n",
                "    rr_sum = 0.0\n",
                "    for qid, ranked_list in query_results.items():\n",
                "        rr_sum += reciprocal_rank(ranked_list, query_relevance.get(qid, set()))\n",
                "    return rr_sum / len(query_results) if query_results else 0.0\n",
                "\n",
                "\n",
                "# Example\n",
                "q_res = {'q1': ['d1', 'd2', 'd3'], 'q2': ['d2', 'd3', 'd1']}\n",
                "q_rel = {'q1': {'d3'}, 'q2': {'d2'}}\n",
                "\n",
                "mrr = mean_reciprocal_rank(q_res, q_rel)\n",
                "print(f\"MRR Score: {mrr:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Normalized Discounted Cumulative Gain (NDCG) <a name=\"ndcg\"></a>\\n\n",
                "\\n\n",
                "NDCG measures the usefulness, or gain, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower ranks.\\n\n",
                "\\n\n",
                "$$ DCG_p = \\sum_{i=1}^{p} \\frac{rel_i}{\\log_2(i+1)} $$\\n\n",
                "\\n\n",
                "$$ NDCG_p = \\frac{DCG_p}{IDCG_p} $$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "NDCG@5: 0.9575\n"
                    ]
                }
            ],
            "source": [
                "def dcg_at_k(r, k):\n",
                "    r = str(r)[:k]\n",
                "    if not r:\n",
                "        return 0.0\n",
                "    return r[0] + sum(rel / math.log2(i + 2) for i, rel in enumerate(r[1:]))\n",
                "    # Standard implementation often uses:\n",
                "    # sum((2^rel - 1) / log2(i + 2))\n",
                "    \n",
                "def calculate_ndcg(ranked_list, relevant_scores, k):\n",
                "    dcg = 0.0\n",
                "    for i, doc_id in enumerate(ranked_list[:k]):\n",
                "        rel = relevant_scores.get(doc_id, 0)\n",
                "        dcg += (2**rel - 1) / math.log2(i + 2)\n",
                "        \n",
                "    # Ideal DCG (sort relevant docs by score descending)\n",
                "    ideal_scores = sorted(relevant_scores.values(), reverse=True)\n",
                "    idcg = 0.0\n",
                "    for i, rel in enumerate(ideal_scores[:k]):\n",
                "        idcg += (2**rel - 1) / math.log2(i + 2)\n",
                "        \n",
                "    if idcg == 0:\n",
                "        return 0.0\n",
                "    return dcg / idcg\n",
                "\n",
                "# Example with Graded Relevance (3=High, 2=Medium, 1=Low, 0=Non-relevant)\n",
                "ranked_docs = ['d1', 'd2', 'd3', 'd4', 'd5']\n",
                "relevance_scores = {'d1': 3, 'd2': 2, 'd3': 3, 'd4': 0, 'd5': 1}\n",
                "\n",
                "ndcg_score = calculate_ndcg(ranked_docs, relevance_scores, k=5)\n",
                "print(f\"NDCG@5: {ndcg_score:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
