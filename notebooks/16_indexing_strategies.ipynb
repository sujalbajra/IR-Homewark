{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 16. Indexing Strategies: BSBI & SPIMI\\n\n",
                "\\n\n",
                "When dealing with millions of documents, we cannot hold the entire index in memory while building it. We need **Disk-Based Index Construction**.\\n\n",
                "\\n\n",
                "We will demonstrate two standard algorithms:\\n\n",
                "1. **BSBI (Block Sort-Based Indexing)**: Sorts (TermID, DocID) pairs.\\n\n",
                "2. **SPIMI (Single-Pass In-Memory Indexing)**: Builds separate dictionaries for blocks and merges them."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processing 60 documents.\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import glob\n",
                "import shutil\n",
                "from collections import defaultdict\n",
                "\n",
                "# Simulating a disk environment\\n\n",
                "DISK_DIR = \"../data/disk_simulation\"\n",
                "if os.path.exists(DISK_DIR):\n",
                "    shutil.rmtree(DISK_DIR)\n",
                "os.makedirs(DISK_DIR)\n",
                "\n",
                "DATA_DIR = \"../data\"\n",
                "files = sorted(glob.glob(os.path.join(DATA_DIR, \"doc*.txt\")))\n",
                "print(f\"Processing {len(files)} documents.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. BSBI (Block Sort-Based Indexing)\\n\n",
                "**Idea**: Map terms to IDs, collect (TermID, DocID) pairs, sort them, and write to disk blocks.\\n\n",
                "\\n\n",
                "*(Since we are in python, we simulate blocks)*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  -> Wrote block ../data/disk_simulation\\bsbi_block_0.txt\n",
                        "  -> Wrote block ../data/disk_simulation\\bsbi_block_1.txt\n",
                        "  -> Wrote block ../data/disk_simulation\\bsbi_block_2.txt\n"
                    ]
                }
            ],
            "source": [
                "class BSBIIndexer:\n",
                "    def __init__(self, block_size=10): # block_size in number of docs for demo\\n\n",
                "        self.term2id = {}\n",
                "        self.id2term = {}\n",
                "        self.current_term_id = 0\n",
                "        self.block_size = block_size\n",
                "        self.block_count = 0\n",
                "        \n",
                "    def get_term_id(self, term):\n",
                "        if term not in self.term2id:\n",
                "            self.term2id[term] = self.current_term_id\n",
                "            self.id2term[self.current_term_id] = term\n",
                "            self.current_term_id += 1\n",
                "        return self.term2id[term]\n",
                "    \n",
                "    def invert_block(self, doc_batch):\n",
                "        # 1. Parse & Convert to IDs\\n\n",
                "        pairs = []\n",
                "        for doc_path in doc_batch:\n",
                "            doc_id = os.path.basename(doc_path)\n",
                "            with open(doc_path, 'r', encoding='utf-8') as f:\n",
                "                tokens = f.read().split()\n",
                "                for token in tokens:\n",
                "                    tid = self.get_term_id(token)\n",
                "                    pairs.append((tid, doc_id))\n",
                "        \n",
                "        # 2. Sort pairs\\n\n",
                "        pairs.sort()\n",
                "        \n",
                "        # 3. Create Posting Lists\\n\n",
                "        block_index = defaultdict(list)\n",
                "        for tid, did in pairs:\n",
                "            if not block_index[tid] or block_index[tid][-1] != did:\n",
                "                block_index[tid].append(did)\n",
                "                \n",
                "        return block_index\n",
                "    \n",
                "    def write_block(self, block_index):\n",
                "        # Write sorted block to disk\\n\n",
                "        filename = os.path.join(DISK_DIR, f\"bsbi_block_{self.block_count}.txt\")\n",
                "        with open(filename, 'w', encoding='utf-8') as f:\n",
                "            for tid in sorted(block_index.keys()):\n",
                "                postings = \",\".join(block_index[tid])\n",
                "                term = self.id2term[tid]\n",
                "                f.write(f\"{term}:{postings}\\n\")\n",
                "        self.block_count += 1\n",
                "        print(f\"  -> Wrote block {filename}\")\n",
                "\n",
                "bsbi = BSBIIndexer(block_size=20)\n",
                "\n",
                "# Process in chunks\\n\n",
                "for i in range(0, len(files), 20):\n",
                "    chunk = files[i:i+20]\n",
                "    index_block = bsbi.invert_block(chunk)\n",
                "    bsbi.write_block(index_block)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. SPIMI (Single-Pass In-Memory Indexing)\\n\n",
                "**Idea**: No TermIDs needed. Just build a dictionary, sort terms *only when block is full*, and write.\\n\n",
                "\\n\n",
                "Advantages:\\n\n",
                "- No need to maintain global TermID mapping (saved memory)\\n\n",
                "- Faster (no sorting of pairs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  -> Wrote SPIMI block ../data/disk_simulation\\spimi_block_0.txt\n",
                        "  -> Wrote SPIMI block ../data/disk_simulation\\spimi_block_1.txt\n",
                        "  -> Wrote SPIMI block ../data/disk_simulation\\spimi_block_2.txt\n"
                    ]
                }
            ],
            "source": [
                "def spimi_invert(doc_batch, block_id):\n",
                "    dictionary = defaultdict(list)\n",
                "    \n",
                "    for doc_path in doc_batch:\n",
                "        doc_id = os.path.basename(doc_path)\n",
                "        with open(doc_path, 'r', encoding='utf-8') as f:\n",
                "            tokens = f.read().split()\n",
                "            for token in tokens:\n",
                "                if not dictionary[token] or dictionary[token][-1] != doc_id:\n",
                "                    dictionary[token].append(doc_id)\n",
                "    \n",
                "    # Sort terms only at write time\\n\n",
                "    sorted_terms = sorted(dictionary.keys())\n",
                "    \n",
                "    filename = os.path.join(DISK_DIR, f\"spimi_block_{block_id}.txt\")\n",
                "    with open(filename, 'w', encoding='utf-8') as f:\n",
                "        for term in sorted_terms:\n",
                "            postings = \",\".join(dictionary[term])\n",
                "            f.write(f\"{term}:{postings}\\n\")\n",
                "    print(f\"  -> Wrote SPIMI block {filename}\")\n",
                "\n",
                "# Process in chunks\\n\n",
                "block_id = 0\n",
                "for i in range(0, len(files), 20):\n",
                "    chunk = files[i:i+20]\n",
                "    spimi_invert(chunk, block_id)\n",
                "    block_id += 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Merging Blocks (The Final Step)\\n\n",
                "Both algorithms end with merging sorted blocks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Merged Index Size: 572 terms.\n"
                    ]
                }
            ],
            "source": [
                "def merge_blocks(pattern):\n",
                "    # Simplified merge: Load all blocks and write final index\\n\n",
                "    # In reality, this would be a k-way merge stream\\n\n",
                "    \n",
                "    final_index = defaultdict(list)\n",
                "    block_files = glob.glob(os.path.join(DISK_DIR, pattern))\n",
                "    \n",
                "    for bf in block_files:\n",
                "        with open(bf, 'r', encoding='utf-8') as f:\n",
                "            for line in f:\n",
                "                term, postings = line.strip().split(':')\n",
                "                doc_list = postings.split(',')\n",
                "                final_index[term].extend(doc_list)\n",
                "                \n",
                "    # Sort postings list one last time (deduplication if needed)\\n\n",
                "    for term in final_index:\n",
                "        final_index[term] = sorted(list(set(final_index[term])))\n",
                "        \n",
                "    return final_index\n",
                "\n",
                "merged_index = merge_blocks(\"spimi_block_*.txt\")\n",
                "print(f\"\\nMerged Index Size: {len(merged_index)} terms.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
