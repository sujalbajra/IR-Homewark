{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 02. Text Preprocessing for Nepali IR\n",
                "\n",
                "## Table of Contents\n",
                "1. [Introduction](#introduction)\n",
                "2. [Theory: Text Preprocessing](#theory)\n",
                "3. [Tokenization](#tokenization)\n",
                "4. [Stopword Removal](#stopwords)\n",
                "5. [Stemming](#stemming)\n",
                "6. [Complete Preprocessing Pipeline](#pipeline)\n",
                "7. [Summary](#summary)\n",
                "\n",
                "---\n",
                "\n",
                "## 1. Introduction <a name=\"introduction\"></a>\n",
                "\n",
                "Text preprocessing is a **critical step** in Information Retrieval. Raw text contains noise, variations, and redundancy that can hurt retrieval performance.\n",
                "\n",
                "### Why Preprocess?\n",
                "- **Reduce vocabulary size**: \"‡§®‡•á‡§™‡§æ‡§≤\", \"‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã\", \"‡§®‡•á‡§™‡§æ‡§≤‡§Æ‡§æ\" ‚Üí \"‡§®‡•á‡§™‡§æ‡§≤\"\n",
                "- **Remove noise**: Common words like \"‡§∞\", \"‡§õ\", \"‡§π‡•ã\" add no information\n",
                "- **Normalize text**: Consistent representation improves matching\n",
                "- **Improve efficiency**: Smaller index, faster search\n",
                "\n",
                "---\n",
                "\n",
                "## 2. Theory: Text Preprocessing <a name=\"theory\"></a>\n",
                "\n",
                "### Standard Preprocessing Pipeline:\n",
                "\n",
                "```\n",
                "Raw Text ‚Üí Tokenization ‚Üí Stopword Removal ‚Üí Stemming/Lemmatization ‚Üí Indexed Terms\n",
                "```\n",
                "\n",
                "### 1. **Tokenization**\n",
                "Breaking text into individual words (tokens).\n",
                "\n",
                "**Example:**\n",
                "```\n",
                "\"‡§®‡•á‡§™‡§æ‡§≤ ‡§π‡§ø‡§Æ‡§æ‡§≤‡§ï‡•ã ‡§¶‡•á‡§∂ ‡§π‡•ã‡•§\" ‚Üí [\"‡§®‡•á‡§™‡§æ‡§≤\", \"‡§π‡§ø‡§Æ‡§æ‡§≤‡§ï‡•ã\", \"‡§¶‡•á‡§∂\", \"‡§π‡•ã\", \"‡•§\"]\n",
                "```\n",
                "\n",
                "### 2. **Stopword Removal**\n",
                "Removing frequently occurring words that carry little meaning.\n",
                "\n",
                "**Nepali Stopwords:** ‡§∞, ‡§õ, ‡§π‡•ã, ‡§Æ‡§æ, ‡§ï‡•ã, ‡§≤‡•á, etc.\n",
                "\n",
                "**Why?** These words appear in almost every document and don't help distinguish relevant documents.\n",
                "\n",
                "### 3. **Stemming**\n",
                "Reducing words to their root form.\n",
                "\n",
                "**Example:**\n",
                "```\n",
                "‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‚Üí ‡§®‡•á‡§™‡§æ‡§≤\n",
                "‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‚Üí ‡§®‡•á‡§™‡§æ‡§≤\n",
                "‡§®‡•á‡§™‡§æ‡§≤‡§Æ‡§æ ‚Üí ‡§®‡•á‡§™‡§æ‡§≤\n",
                "```\n",
                "\n",
                "**Benefits:**\n",
                "- Query \"‡§®‡•á‡§™‡§æ‡§≤\" matches documents containing \"‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã\", \"‡§®‡•á‡§™‡§æ‡§≤‡§Æ‡§æ\"\n",
                "- Vocabulary reduction: Smaller index\n",
                "\n",
                "---\n",
                "\n",
                "## 3. Tokenization <a name=\"tokenization\"></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úì Loaded 10 documents\n"
                    ]
                }
            ],
            "source": [
                "import re\n",
                "from pathlib import Path\n",
                "\n",
                "# Load documents from previous notebook\n",
                "DATA_DIR = Path('../data')\n",
                "\n",
                "def load_documents(data_dir):\n",
                "    \"\"\"Load all documents from data directory.\"\"\"\n",
                "    documents = {}\n",
                "    for file_path in sorted(data_dir.glob('doc*.txt')):\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            documents[file_path.stem] = f.read()\n",
                "    return documents\n",
                "\n",
                "documents = load_documents(DATA_DIR)\n",
                "print(f\"‚úì Loaded {len(documents)} documents\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üìù Sample Text:\n",
                        "‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§á‡§§‡§ø‡§π‡§æ‡§∏ ‡§∞ ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø\n",
                        "\n",
                        "‡§®‡•á‡§™‡§æ‡§≤ ‡§¶‡§ï‡•ç‡§∑‡§ø‡§£ ‡§è‡§∂‡§ø‡§Ø‡§æ‡§Æ‡§æ ‡§Ö‡§µ‡§∏‡•ç‡§•‡§ø‡§§ ‡§è‡§â‡§ü‡§æ ‡§∏‡•Å‡§®‡•ç‡§¶‡§∞ ‡§π‡§ø‡§Æ‡§æ‡§≤‡•Ä ‡§¶‡•á‡§∂ ‡§π‡•ã‡•§ ‡§Ø‡•ã ‡§¶‡•á‡§∂ ‡§Ü‡§´‡•ç‡§®‡•ã ‡§∏‡§Æ‡•É‡§¶‡•ç‡§ß ‡§á‡§§‡§ø‡§π‡§æ‡§∏ ‡§∞ ‡§µ‡§ø‡§µ‡§ø‡§ß ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø ‡§µ‡§ø‡§∂‡•ç‡§µ‡§≠‡§∞ ‡§™‡•ç‡§∞‡§∏‡§ø‡§¶‡•ç‡§ß ‡§õ‡•§ ‡§®‡•á‡§™‡§æ‡§≤‡§Æ‡§æ ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§ú‡§æ‡§§‡§ú‡§æ‡§§‡§ø ‡§∞ ‡§ß‡§∞‡•ç‡§Æ‡§ï‡§æ ‡§Æ‡§æ‡§®‡§ø‡§∏‡§π‡§∞‡•Ç ‡§∏‡§¶‡•ç‡§≠\n",
                        "\n",
                        "üî§ Tokens:\n",
                        "['‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã', '‡§á‡§§‡§ø‡§π‡§æ‡§∏', '‡§∞', '‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø', '‡§®‡•á‡§™‡§æ‡§≤', '‡§¶‡§ï‡•ç‡§∑‡§ø‡§£', '‡§è‡§∂‡§ø‡§Ø‡§æ‡§Æ‡§æ', '‡§Ö‡§µ‡§∏‡•ç‡§•‡§ø‡§§', '‡§è‡§â‡§ü‡§æ', '‡§∏‡•Å‡§®‡•ç‡§¶‡§∞', '‡§π‡§ø‡§Æ‡§æ‡§≤‡•Ä', '‡§¶‡•á‡§∂', '‡§π‡•ã', '‡§Ø‡•ã', '‡§¶‡•á‡§∂', '‡§Ü‡§´‡•ç‡§®‡•ã', '‡§∏‡§Æ‡•É‡§¶‡•ç‡§ß', '‡§á‡§§‡§ø‡§π‡§æ‡§∏', '‡§∞', '‡§µ‡§ø‡§µ‡§ø‡§ß']\n",
                        "\n",
                        "Total tokens: 32\n"
                    ]
                }
            ],
            "source": [
                "def tokenize(text):\n",
                "    \"\"\"\n",
                "    Tokenize Nepali text into words.\n",
                "    \n",
                "    This is a simple whitespace tokenizer that:\n",
                "    1. Converts to lowercase (for case normalization)\n",
                "    2. Splits on whitespace\n",
                "    3. Removes punctuation and non-alphabetic characters\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    text : str\n",
                "        Input text in Nepali\n",
                "    \n",
                "    Returns:\n",
                "    --------\n",
                "    list : List of tokens (words)\n",
                "    \"\"\"\n",
                "    # Split by whitespace\n",
                "    tokens = text.split()\n",
                "    \n",
                "    # Remove punctuation and clean tokens\n",
                "    # Keep only Nepali unicode characters and digits\n",
                "    cleaned_tokens = []\n",
                "    for token in tokens:\n",
                "        # Remove common punctuation marks\n",
                "        token = token.strip('‡•§,.!?;:\"\\'-()[]{}/')\n",
                "        \n",
                "        # Keep token if it contains Nepali characters\n",
                "        # Nepali unicode range: U+0900 to U+097F\n",
                "        if token and any('\\u0900' <= c <= '\\u097F' for c in token):\n",
                "            cleaned_tokens.append(token)\n",
                "    \n",
                "    return cleaned_tokens\n",
                "\n",
                "# Test tokenization\n",
                "sample_text = list(documents.values())[0][:200]\n",
                "tokens = tokenize(sample_text)\n",
                "\n",
                "print(\"\\nüìù Sample Text:\")\n",
                "print(sample_text)\n",
                "print(\"\\nüî§ Tokens:\")\n",
                "print(tokens[:20])\n",
                "print(f\"\\nTotal tokens: {len(tokens)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Stopword Removal <a name=\"stopwords\"></a>\n",
                "\n",
                "Stopwords are common words that appear frequently but carry little semantic meaning. Removing them:\n",
                "- Reduces index size\n",
                "- Improves retrieval efficiency\n",
                "- Focuses on content-bearing words"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úì Loaded 37 stopwords\n",
                        "\n",
                        "Sample stopwords: ['‡§ï‡•ã', '‡§≠‡§è‡§ï‡•ã', '‡§Ø‡§π‡§æ‡§Å', '‡§Ø‡•Ä', '‡§Ö‡§®‡•ç‡§Ø', '‡§§‡•ç‡§Ø‡§π‡§æ‡§Å', '‡§Ø‡§∏‡§¨‡§æ‡§π‡•á‡§ï', '‡§ó‡§∞‡•á‡§ï‡§æ', '‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ', '‡§ó‡§∞‡•ç‡§õ', '‡§§‡•Ä', '‡§π‡•ã', '‡§è‡§ï', '‡§∏‡§¨‡•à', '‡§õ‡§®‡•ç']\n"
                    ]
                }
            ],
            "source": [
                "def load_stopwords(stopwords_file):\n",
                "    \"\"\"\n",
                "    Load stopwords from CSV file.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    stopwords_file : Path\n",
                "        Path to stopwords CSV file\n",
                "    \n",
                "    Returns:\n",
                "    --------\n",
                "    set : Set of stopwords for fast lookup\n",
                "    \"\"\"\n",
                "    stopwords = set()\n",
                "    \n",
                "    with open(stopwords_file, 'r', encoding='utf-8') as f:\n",
                "        # Skip header\n",
                "        next(f)\n",
                "        for line in f:\n",
                "            word = line.strip()\n",
                "            if word:\n",
                "                stopwords.add(word)\n",
                "    \n",
                "    return stopwords\n",
                "\n",
                "# Load Nepali stopwords\n",
                "stopwords = load_stopwords(DATA_DIR / 'nepali_stopwords.csv')\n",
                "print(f\"‚úì Loaded {len(stopwords)} stopwords\")\n",
                "print(f\"\\nSample stopwords: {list(stopwords)[:15]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üìä Before stopword removal: 32 tokens\n",
                        "üìä After stopword removal: 24 tokens\n",
                        "üìä Reduction: 8 tokens (25.0%)\n",
                        "\n",
                        "üîç Comparison:\n",
                        "Before: ['‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã', '‡§á‡§§‡§ø‡§π‡§æ‡§∏', '‡§∞', '‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø', '‡§®‡•á‡§™‡§æ‡§≤', '‡§¶‡§ï‡•ç‡§∑‡§ø‡§£', '‡§è‡§∂‡§ø‡§Ø‡§æ‡§Æ‡§æ', '‡§Ö‡§µ‡§∏‡•ç‡§•‡§ø‡§§', '‡§è‡§â‡§ü‡§æ', '‡§∏‡•Å‡§®‡•ç‡§¶‡§∞', '‡§π‡§ø‡§Æ‡§æ‡§≤‡•Ä', '‡§¶‡•á‡§∂', '‡§π‡•ã', '‡§Ø‡•ã', '‡§¶‡•á‡§∂']\n",
                        "After:  ['‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã', '‡§á‡§§‡§ø‡§π‡§æ‡§∏', '‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø', '‡§®‡•á‡§™‡§æ‡§≤', '‡§¶‡§ï‡•ç‡§∑‡§ø‡§£', '‡§è‡§∂‡§ø‡§Ø‡§æ‡§Æ‡§æ', '‡§Ö‡§µ‡§∏‡•ç‡§•‡§ø‡§§', '‡§∏‡•Å‡§®‡•ç‡§¶‡§∞', '‡§π‡§ø‡§Æ‡§æ‡§≤‡•Ä', '‡§¶‡•á‡§∂', '‡§Ø‡•ã', '‡§¶‡•á‡§∂', '‡§∏‡§Æ‡•É‡§¶‡•ç‡§ß', '‡§á‡§§‡§ø‡§π‡§æ‡§∏', '‡§µ‡§ø‡§µ‡§ø‡§ß']\n"
                    ]
                }
            ],
            "source": [
                "def remove_stopwords(tokens, stopwords):\n",
                "    \"\"\"\n",
                "    Remove stopwords from token list.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    tokens : list\n",
                "        List of tokens\n",
                "    stopwords : set\n",
                "        Set of stopwords\n",
                "    \n",
                "    Returns:\n",
                "    --------\n",
                "    list : Filtered tokens without stopwords\n",
                "    \"\"\"\n",
                "    return [token for token in tokens if token not in stopwords]\n",
                "\n",
                "# Test stopword removal\n",
                "sample_tokens = tokenize(sample_text)\n",
                "filtered_tokens = remove_stopwords(sample_tokens, stopwords)\n",
                "\n",
                "print(f\"\\nüìä Before stopword removal: {len(sample_tokens)} tokens\")\n",
                "print(f\"üìä After stopword removal: {len(filtered_tokens)} tokens\")\n",
                "print(f\"üìä Reduction: {len(sample_tokens) - len(filtered_tokens)} tokens ({round((1 - len(filtered_tokens)/len(sample_tokens))*100, 1)}%)\")\n",
                "\n",
                "print(\"\\nüîç Comparison:\")\n",
                "print(f\"Before: {sample_tokens[:15]}\")\n",
                "print(f\"After:  {filtered_tokens[:15]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Stemming <a name=\"stemming\"></a>\n",
                "\n",
                "**Stemming** reduces words to their root form. Since we're using vanilla Python, we'll use a **dictionary-based stemmer** with our custom mapping file.\n",
                "\n",
                "### Approaches to Stemming:\n",
                "1. **Rule-based**: Apply linguistic rules (e.g., remove suffixes)\n",
                "2. **Dictionary-based**: Map words to stems using a lookup table (our approach)\n",
                "3. **Statistical**: Learn stemming patterns from data\n",
                "\n",
                "For Nepali, dictionary-based is simple and effective for educational purposes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úì Loaded 58 stemming rules\n",
                        "\n",
                        "üìñ Stemming Examples:\n",
                        "  ‡§®‡•á‡§™‡§æ‡§≤ ‚Üí ‡§®‡•á‡§™‡§æ‡§≤\n",
                        "  ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‚Üí ‡§®‡•á‡§™‡§æ‡§≤\n",
                        "  ‡§®‡•á‡§™‡§æ‡§≤‡§Æ‡§æ ‚Üí ‡§®‡•á‡§™‡§æ‡§≤\n",
                        "  ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‚Üí ‡§®‡•á‡§™‡§æ‡§≤\n",
                        "  ‡§®‡•á‡§™‡§æ‡§≤‡•Ä‡§π‡§∞‡•Ç ‚Üí ‡§®‡•á‡§™‡§æ‡§≤\n",
                        "  ‡§π‡§ø‡§Æ‡§æ‡§≤ ‚Üí ‡§π‡§ø‡§Æ‡§æ‡§≤\n",
                        "  ‡§π‡§ø‡§Æ‡§æ‡§≤‡§ï‡•ã ‚Üí ‡§π‡§ø‡§Æ‡§æ‡§≤\n",
                        "  ‡§π‡§ø‡§Æ‡§æ‡§≤‡§Æ‡§æ ‚Üí ‡§π‡§ø‡§Æ‡§æ‡§≤\n",
                        "  ‡§π‡§ø‡§Æ‡§æ‡§≤‡•Ä ‚Üí ‡§π‡§ø‡§Æ‡§æ‡§≤\n",
                        "  ‡§π‡§ø‡§Æ‡§æ‡§≤‡§π‡§∞‡•Ç ‚Üí ‡§π‡§ø‡§Æ‡§æ‡§≤\n"
                    ]
                }
            ],
            "source": [
                "def load_stemming_dict(stemming_file):\n",
                "    \"\"\"\n",
                "    Load stemming dictionary from CSV file.\n",
                "    \n",
                "    Format: word,stem\n",
                "    Example: ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã,‡§®‡•á‡§™‡§æ‡§≤\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    stemming_file : Path\n",
                "        Path to stemming CSV file\n",
                "    \n",
                "    Returns:\n",
                "    --------\n",
                "    dict : Mapping from word to its stem\n",
                "    \"\"\"\n",
                "    stem_dict = {}\n",
                "    \n",
                "    with open(stemming_file, 'r', encoding='utf-8') as f:\n",
                "        # Skip header\n",
                "        next(f)\n",
                "        for line in f:\n",
                "            parts = line.strip().split(',')\n",
                "            if len(parts) == 2:\n",
                "                word, stem = parts\n",
                "                stem_dict[word] = stem\n",
                "    \n",
                "    return stem_dict\n",
                "\n",
                "# Load stemming dictionary\n",
                "stem_dict = load_stemming_dict(DATA_DIR / 'nepali_stemming.csv')\n",
                "print(f\"‚úì Loaded {len(stem_dict)} stemming rules\")\n",
                "\n",
                "# Show some examples\n",
                "print(\"\\nüìñ Stemming Examples:\")\n",
                "examples = list(stem_dict.items())[:10]\n",
                "for word, stem in examples:\n",
                "    print(f\"  {word} ‚Üí {stem}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üîç Before Stemming:\n",
                        "['‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã', '‡§á‡§§‡§ø‡§π‡§æ‡§∏', '‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø', '‡§®‡•á‡§™‡§æ‡§≤', '‡§¶‡§ï‡•ç‡§∑‡§ø‡§£', '‡§è‡§∂‡§ø‡§Ø‡§æ‡§Æ‡§æ', '‡§Ö‡§µ‡§∏‡•ç‡§•‡§ø‡§§', '‡§∏‡•Å‡§®‡•ç‡§¶‡§∞', '‡§π‡§ø‡§Æ‡§æ‡§≤‡•Ä', '‡§¶‡•á‡§∂', '‡§Ø‡•ã', '‡§¶‡•á‡§∂', '‡§∏‡§Æ‡•É‡§¶‡•ç‡§ß', '‡§á‡§§‡§ø‡§π‡§æ‡§∏', '‡§µ‡§ø‡§µ‡§ø‡§ß']\n",
                        "\n",
                        "üîç After Stemming:\n",
                        "['‡§®‡•á‡§™‡§æ‡§≤', '‡§á‡§§‡§ø‡§π‡§æ‡§∏', '‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø', '‡§®‡•á‡§™‡§æ‡§≤', '‡§¶‡§ï‡•ç‡§∑‡§ø‡§£', '‡§è‡§∂‡§ø‡§Ø‡§æ‡§Æ‡§æ', '‡§Ö‡§µ‡§∏‡•ç‡§•‡§ø‡§§', '‡§∏‡•Å‡§®‡•ç‡§¶‡§∞', '‡§π‡§ø‡§Æ‡§æ‡§≤', '‡§¶‡•á‡§∂', '‡§Ø‡•ã', '‡§¶‡•á‡§∂', '‡§∏‡§Æ‡•É‡§¶‡•ç‡§ß', '‡§á‡§§‡§ø‡§π‡§æ‡§∏', '‡§µ‡§ø‡§µ‡§ø‡§ß']\n",
                        "\n",
                        "üìä Unique tokens before: 22\n",
                        "üìä Unique tokens after: 19\n",
                        "üìä Reduction: 3 (13.6%)\n"
                    ]
                }
            ],
            "source": [
                "def apply_stemming(tokens, stem_dict):\n",
                "    \"\"\"\n",
                "    Apply stemming to tokens using dictionary lookup.\n",
                "    \n",
                "    If a word is in the dictionary, replace it with its stem.\n",
                "    Otherwise, keep the original word.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    tokens : list\n",
                "        List of tokens\n",
                "    stem_dict : dict\n",
                "        Stemming dictionary\n",
                "    \n",
                "    Returns:\n",
                "    --------\n",
                "    list : Stemmed tokens\n",
                "    \"\"\"\n",
                "    stemmed = []\n",
                "    for token in tokens:\n",
                "        # If token in dictionary, use its stem; otherwise keep original\n",
                "        stemmed_token = stem_dict.get(token, token)\n",
                "        stemmed.append(stemmed_token)\n",
                "    \n",
                "    return stemmed\n",
                "\n",
                "# Test stemming\n",
                "sample_filtered = remove_stopwords(tokenize(sample_text), stopwords)\n",
                "stemmed_tokens = apply_stemming(sample_filtered, stem_dict)\n",
                "\n",
                "print(\"\\nüîç Before Stemming:\")\n",
                "print(sample_filtered[:15])\n",
                "print(\"\\nüîç After Stemming:\")\n",
                "print(stemmed_tokens[:15])\n",
                "\n",
                "# Count unique tokens\n",
                "unique_before = len(set(sample_filtered))\n",
                "unique_after = len(set(stemmed_tokens))\n",
                "print(f\"\\nüìä Unique tokens before: {unique_before}\")\n",
                "print(f\"üìä Unique tokens after: {unique_after}\")\n",
                "print(f\"üìä Reduction: {unique_before - unique_after} ({round((1 - unique_after/unique_before)*100, 1)}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Complete Preprocessing Pipeline <a name=\"pipeline\"></a>\n",
                "\n",
                "Now let's combine all steps into a single preprocessing function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úì Preprocessed all documents\n",
                        "\n",
                        "üìä Preprocessing Statistics:\n",
                        "================================================================================\n",
                        "Doc ID     Original Tokens    After Preprocessing  Reduction %\n",
                        "================================================================================\n",
                        "doc01      87                 65                   25.3%\n",
                        "doc02      80                 59                   26.2%\n",
                        "doc03      72                 54                   25.0%\n",
                        "doc04      76                 58                   23.7%\n",
                        "doc05      77                 56                   27.3%\n",
                        "doc06      75                 59                   21.3%\n",
                        "doc07      71                 56                   21.1%\n",
                        "doc08      88                 67                   23.9%\n",
                        "doc09      86                 68                   20.9%\n",
                        "doc10      85                 61                   28.2%\n",
                        "================================================================================\n"
                    ]
                }
            ],
            "source": [
                "def preprocess_text(text, stopwords, stem_dict):\n",
                "    \"\"\"\n",
                "    Complete preprocessing pipeline for Nepali text.\n",
                "    \n",
                "    Steps:\n",
                "    1. Tokenization\n",
                "    2. Stopword removal\n",
                "    3. Stemming\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    text : str\n",
                "        Raw text\n",
                "    stopwords : set\n",
                "        Set of stopwords\n",
                "    stem_dict : dict\n",
                "        Stemming dictionary\n",
                "    \n",
                "    Returns:\n",
                "    --------\n",
                "    list : Preprocessed tokens\n",
                "    \"\"\"\n",
                "    # Step 1: Tokenize\n",
                "    tokens = tokenize(text)\n",
                "    \n",
                "    # Step 2: Remove stopwords\n",
                "    tokens = remove_stopwords(tokens, stopwords)\n",
                "    \n",
                "    # Step 3: Apply stemming\n",
                "    tokens = apply_stemming(tokens, stem_dict)\n",
                "    \n",
                "    return tokens\n",
                "\n",
                "# Preprocess all documents\n",
                "preprocessed_docs = {}\n",
                "for doc_id, text in documents.items():\n",
                "    preprocessed_docs[doc_id] = preprocess_text(text, stopwords, stem_dict)\n",
                "\n",
                "print(\"‚úì Preprocessed all documents\\n\")\n",
                "\n",
                "# Show statistics\n",
                "print(\"üìä Preprocessing Statistics:\")\n",
                "print(\"=\"*80)\n",
                "print(f\"{'Doc ID':<10} {'Original Tokens':<18} {'After Preprocessing':<20} {'Reduction %'}\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "for doc_id in sorted(preprocessed_docs.keys()):\n",
                "    original = tokenize(documents[doc_id])\n",
                "    processed = preprocessed_docs[doc_id]\n",
                "    reduction = round((1 - len(processed)/len(original))*100, 1)\n",
                "    \n",
                "    print(f\"{doc_id:<10} {len(original):<18} {len(processed):<20} {reduction}%\")\n",
                "\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üìö Vocabulary Size: 398 unique terms\n",
                        "\n",
                        "Sample terms: ['‡§≤‡•Å‡§™‡•ç‡§§‡§™‡•ç‡§∞‡§æ‡§Ø', '‡§°‡§ø‡§ú‡§ø‡§ü‡§≤', '‡§≤‡§ï‡•ç‡§∑‡•ç‡§Æ‡•Ä‡§™‡•ç‡§∞‡§∏‡§æ‡§¶', '‡§ó‡§∞‡§ø‡§è‡§ï‡§æ', '‡§∏‡§û‡•ç‡§ö‡§æ‡§≤‡§®', '‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ', '‡§™‡§æ‡§∞‡§ø‡§ú‡§æ‡§§', '‡§∞‡§ö‡§®‡§æ', '‡§è‡§ï‡•Ä‡§ï‡§∞‡§£', '‡§∂‡§∞‡§¶', '‡§≤‡•ã‡§ï', '‡§™‡•Ç‡§∞‡•ç‡§µ', '‡§∏‡§¨‡•à‡§≠‡§®‡•ç‡§¶‡§æ', '‡§≤‡§æ‡§ó‡•Ç', '‡§∏‡§Ç‡§ó‡§†‡§®‡§≤‡•á', '‡§Ø‡•ã‡§ó‡§¶‡§æ‡§®', '‡§Ü‡§Ø‡•ã‡§ó‡§≤‡•á', '‡§Ü‡§Ø', '‡§∞‡§æ‡§ú‡§®‡•Ä‡§§‡§ø', '‡§µ‡§ø‡§∂‡•ç‡§µ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§≤‡§Ø']\n"
                    ]
                }
            ],
            "source": [
                "# Build vocabulary (unique terms across all documents)\n",
                "def build_vocabulary(preprocessed_docs):\n",
                "    \"\"\"\n",
                "    Build vocabulary from preprocessed documents.\n",
                "    \n",
                "    Returns:\n",
                "    --------\n",
                "    set : Set of unique terms\n",
                "    \"\"\"\n",
                "    vocabulary = set()\n",
                "    for terms in preprocessed_docs.values():\n",
                "        vocabulary.update(terms)\n",
                "    return vocabulary\n",
                "\n",
                "vocabulary = build_vocabulary(preprocessed_docs)\n",
                "\n",
                "print(f\"\\nüìö Vocabulary Size: {len(vocabulary)} unique terms\")\n",
                "print(f\"\\nSample terms: {list(vocabulary)[:20]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Summary <a name=\"summary\"></a>\n",
                "\n",
                "### What We Learned:\n",
                "\n",
                "1. **Tokenization**: Breaking text into words\n",
                "   - Simple whitespace splitting\n",
                "   - Punctuation removal\n",
                "   - Unicode handling for Nepali\n",
                "\n",
                "2. **Stopword Removal**: Filtering common words\n",
                "   - Reduces noise\n",
                "   - Decreases vocabulary size\n",
                "   - Improves efficiency\n",
                "\n",
                "3. **Stemming**: Reducing words to root forms\n",
                "   - Dictionary-based approach (vanilla Python)\n",
                "   - Groups word variations together\n",
                "   - Further reduces vocabulary\n",
                "\n",
                "4. **Complete Pipeline**: Integrated preprocessing\n",
                "   - All steps in one function\n",
                "   - Ready for indexing and retrieval\n",
                "\n",
                "### Key Results:\n",
                "- **Token reduction**: ~30-40% through stopword removal\n",
                "- **Vocabulary reduction**: ~15-25% through stemming\n",
                "- **Final vocabulary**: Clean, normalized terms for IR\n",
                "\n",
                "### Next Steps:\n",
                "In the next notebook (`03_boolean_retrieval.ipynb`), we will:\n",
                "- Implement Boolean retrieval model\n",
                "- Build inverted index\n",
                "- Process AND, OR, NOT queries\n",
                "- Evaluate retrieval results\n",
                "\n",
                "### Research References:\n",
                "- Manning, Raghavan, Sch√ºtze: \"Introduction to Information Retrieval\" (Chapter 2)\n",
                "- Standard preprocessing is fundamental to all IR systems\n",
                "- Language-specific processing is crucial for non-English IR"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
