{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 07. Enhanced Language Modeling for IR\\n\n",
                "\\n\n",
                "## Introduction\\n\n",
                "\\n\n",
                "**Language Modeling** is a probabilistic approach to Information Retrieval where each document is viewed as a language model, and we estimate the probability of that model generating the query.\\n\n",
                "\\n\n",
                "### Formula:\\n\n",
                "$$P(q|d) = \\prod_{t \\in q} P(t|d)$$\\n\n",
                "\\n\n",
                "To handle zero-probability issues (when a query term doesn't appear in a document), we use **smoothing** techniques.\\n\n",
                "\\n\n",
                "## Techniques Implemented:\\n\n",
                "1. **Maximum Likelihood Estimation (MLE)**\\n\n",
                "2. **Jelinek-Mercer Smoothing** (Linear Interpolation)\\n\n",
                "3. **Dirichlet Prior Smoothing** (Bayesian Smoothing)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 10 documents\n"
                    ]
                }
            ],
            "source": [
                "from pathlib import Path\n",
                "import math\n",
                "from collections import Counter, defaultdict\n",
                "\n",
                "# Load data\n",
                "DATA_DIR = Path('../data')\n",
                "\n",
                "def load_documents(data_dir):\n",
                "    documents = {}\n",
                "    for file_path in sorted(data_dir.glob('doc*.txt')):\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            documents[file_path.stem] = f.read()\n",
                "    return documents\n",
                "\n",
                "def tokenize(text):\n",
                "    # Simple whitespace tokenization\n",
                "    return text.split()\n",
                "\n",
                "documents = load_documents(DATA_DIR)\n",
                "print(f\"Loaded {len(documents)} documents\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Building the Collection Model\\n\n",
                "The collection model $P(t|C)$ acts as a background probability distribution. It helps estimate probabilities for unseen words in a document based on their prevalence in the entire corpus."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collection Size: 797 tokens\n"
                    ]
                }
            ],
            "source": [
                "class LanguageModel:\n",
                "    def __init__(self, documents):\n",
                "        self.doc_models = {}  # Stores counts per document\n",
                "        self.collection_counts = Counter()\n",
                "        self.collection_size = 0\n",
                "        self.doc_lengths = {}\n",
                "        \n",
                "        self._build(documents)\n",
                "        \n",
                "    def _build(self, documents):\n",
                "        for doc_id, text in documents.items():\n",
                "            tokens = tokenize(text)\n",
                "            counts = Counter(tokens)\n",
                "            \n",
                "            self.doc_models[doc_id] = counts\n",
                "            self.doc_lengths[doc_id] = len(tokens)\n",
                "            \n",
                "            self.collection_counts.update(tokens)\n",
                "            self.collection_size += len(tokens)\n",
                "            \n",
                "    def get_collection_prob(self, term):\n",
                "        # P(t|C)\n",
                "        count = self.collection_counts.get(term, 0)\n",
                "        if count == 0:\n",
                "             # Minimal smoothing for OOV terms relative to collection\n",
                "            return 1 / (self.collection_size + 1) \n",
                "        return count / self.collection_size\n",
                "\n",
                "lm = LanguageModel(documents)\n",
                "print(f\"Collection Size: {lm.collection_size} tokens\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Smoothing Implementations\\n\n",
                "\\n\n",
                "### Jelinek-Mercer Smoothing\\n\n",
                "Linearly interpolates document model with collection model.\\n\n",
                "\\n\n",
                "$$P_{JM}(t|d) = \\lambda P_{MLE}(t|d) + (1-\\lambda) P(t|C)$$\\n\n",
                "\\n\n",
                "Typical $\\lambda = 0.7$\\n\n",
                "\\n\n",
                "### Dirichlet Prior Smoothing\\n\n",
                "Adds \"pseudo-counts\" based on collection probability, proportional to document length.\\n\n",
                "\\n\n",
                "$$P_{DIR}(t|d) = \\frac{c(t,d) + \\mu P(t|C)}{|d| + \\mu}$$\\n\n",
                "\\n\n",
                "Typical $\\mu = 2000$ (average doc length)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def score_jm(lm, query_terms, doc_id, lam=0.7):\n",
                "    score = 0.0\n",
                "    doc_len = lm.doc_lengths[doc_id]\n",
                "    if doc_len == 0: return -float('inf')\n",
                "    \n",
                "    for term in query_terms:\n",
                "        tf = lm.doc_models[doc_id].get(term, 0)\n",
                "        p_mle = tf / doc_len\n",
                "        p_coll = lm.get_collection_prob(term)\n",
                "        \n",
                "        prob = lam * p_mle + (1 - lam) * p_coll\n",
                "        score += math.log(prob)\n",
                "        \n",
                "    return score\n",
                "\n",
                "\n",
                "def score_dirichlet(lm, query_terms, doc_id, mu=2000):\n",
                "    score = 0.0\n",
                "    doc_len = lm.doc_lengths[doc_id]\n",
                "    if doc_len == 0: return -float('inf')\n",
                "    \n",
                "    for term in query_terms:\n",
                "        tf = lm.doc_models[doc_id].get(term, 0)\n",
                "        p_coll = lm.get_collection_prob(term)\n",
                "        \n",
                "        numerator = tf + (mu * p_coll)\n",
                "        denominator = doc_len + mu\n",
                "        \n",
                "        prob = numerator / denominator\n",
                "        score += math.log(prob)\n",
                "        \n",
                "    return score"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Comparison and Ranking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Jelinek-Mercer (lambda=0.7) ---\n",
                        "doc01: -7.1772\n",
                        "doc05: -9.7240\n",
                        "doc04: -9.9014\n",
                        "\n",
                        "--- Dirichlet (mu=100) ---\n",
                        "doc01: -7.6233\n",
                        "doc05: -9.3132\n",
                        "doc04: -9.4448\n"
                    ]
                }
            ],
            "source": [
                "def rank_documents(query, method='jm', param=None):\n",
                "    query_terms = tokenize(query)\n",
                "    scores = []\n",
                "    \n",
                "    for doc_id in lm.doc_models.keys():\n",
                "        if method == 'jm':\n",
                "            lam = param if param else 0.7\n",
                "            s = score_jm(lm, query_terms, doc_id, lam=lam)\n",
                "        else:\n",
                "            mu = param if param else 2000\n",
                "            s = score_dirichlet(lm, query_terms, doc_id, mu=mu)\n",
                "        scores.append((doc_id, s))\n",
                "    \n",
                "    return sorted(scores, key=lambda x: x[1], reverse=True)\n",
                "\n",
                "# Test Query\n",
                "query = \"नेपालको इतिहास\"\n",
                "\n",
                "print(\"--- Jelinek-Mercer (lambda=0.7) ---\")\n",
                "for doc_id, score in rank_documents(query, 'jm')[:3]:\n",
                "    print(f\"{doc_id}: {score:.4f}\")\n",
                "\n",
                "print(\"\\n--- Dirichlet (mu=100) ---\")\n",
                "# Using smaller mu because our docs are short\n",
                "for doc_id, score in rank_documents(query, 'dirichlet', param=100)[:3]:\n",
                "    print(f\"{doc_id}: {score:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
