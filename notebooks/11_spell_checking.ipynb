{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 11. Spell Checking and Correction\\n\n",
                "\\n\n",
                "In this notebook, we implement mechanisms to handle user typos and spelling errors.\\n\n",
                "\\n\n",
                "## Techniques Covered:\\n\n",
                "1. **Minimum Edit Distance (Levenshtein)**: Calculating the number of operations to transform one string to another.\\n\n",
                "2. **N-gram Overlap**: Finding similar words based on shared character n-grams.\\n\n",
                "3. **Simple Spelling Corrector**: Suggesting corrections for unknown words."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import glob\n",
                "from collections import Counter, defaultdict"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Preparation\\n\n",
                "We need a vocabulary of correct words. We will build this from our document collection."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Vocabulary size: 457 unique words\n",
                        "Top 10 words: [('र', 50), ('छ।', 31), ('नेपालको', 15), ('नेपालमा', 15), ('छन्।', 15), ('हो।', 13), ('पनि', 13), ('हुन्।', 10), ('नेपाल', 9), ('नेपाली', 9)]\n"
                    ]
                }
            ],
            "source": [
                "def load_vocabulary(data_dir=\"../data\"):\n",
                "    vocab = Counter()\n",
                "    for filepath in glob.glob(os.path.join(data_dir, \"*.txt\")):\n",
                "        with open(filepath, 'r', encoding='utf-8') as f:\n",
                "            text = f.read()\n",
                "            # Simple tokenization\n",
                "            tokens = text.lower().split()\n",
                "            # Filter purely numeric tokens and update counts\n",
                "            valid_tokens = [t for t in tokens if not t.isnumeric()]\n",
                "            vocab.update(valid_tokens)\n",
                "    return vocab\n",
                "\n",
                "vocabulary = load_vocabulary()\n",
                "print(f\"Vocabulary size: {len(vocabulary)} unique words\")\n",
                "print(\"Top 10 words:\", vocabulary.most_common(10))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Minimum Edit Distance (Levenshtein)\\n\n",
                "\\n\n",
                "The edit distance between two strings is the minimum number of operations (insertions, deletions, substitutions) required to change one string into the other.\\n\n",
                "\\n\n",
                "We use dynamic programming to calculate this efficiently."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Distance('kitten', 'sitting'): 3\n",
                        "Distance('nepal', 'nipal'): 1\n"
                    ]
                }
            ],
            "source": [
                "def simple_edit_distance(s1, s2):\n",
                "    m = len(s1)\n",
                "    n = len(s2)\n",
                "    \n",
                "    # Initialize matrix\n",
                "    # dp[i][j] stores distance between s1[:i] and s2[:j]\n",
                "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
                "    \n",
                "    for i in range(m + 1):\n",
                "        dp[i][0] = i\n",
                "    for j in range(n + 1):\n",
                "        dp[0][j] = j\n",
                "        \n",
                "    for i in range(1, m + 1):\n",
                "        for j in range(1, n + 1):\n",
                "            if s1[i-1] == s2[j-1]:\n",
                "                cost = 0\n",
                "            else:\n",
                "                cost = 1 # Substitution cost\n",
                "            \n",
                "            dp[i][j] = min(\n",
                "                dp[i-1][j] + 1,      # Deletion\n",
                "                dp[i][j-1] + 1,      # Insertion\n",
                "                dp[i-1][j-1] + cost  # Substitution\n",
                "            )\n",
                "            \n",
                "    return dp[m][n]\n",
                "\n",
                "# Test\n",
                "print(f\"Distance('kitten', 'sitting'): {simple_edit_distance('kitten', 'sitting')}\")\n",
                "print(f\"Distance('nepal', 'nipal'): {simple_edit_distance('nepal', 'nipal')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. N-gram Based Correction\\n\n",
                "\\n\n",
                "For finding candidate corrections efficiently, we can use character n-grams. Words that share many n-grams are likely similar.\\n\n",
                "\\n\n",
                "$$ Jaccard(A, B) = \\frac{|A \\cap B|}{|A \\cup B|} $$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Suggestions for 'नेपल':\n",
                        "[('नेपाल', 0.5714285714285714), ('नेपाली', 0.3333333333333333), ('नेपालको', 0.3), ('नेपालमा', 0.3), ('नेपालका', 0.3)]\n"
                    ]
                }
            ],
            "source": [
                "def get_char_ngrams(text, n=2):\n",
                "    # Add padding\n",
                "    padded = f\"${text}$\"\n",
                "    return [padded[i:i+n] for i in range(len(padded)-n+1)]\n",
                "\n",
                "def jaccard_similarity(set1, set2):\n",
                "    intersection = len(set1.intersection(set2))\n",
                "    union = len(set1.union(set2))\n",
                "    return intersection / union if union > 0 else 0.0\n",
                "\n",
                "def get_ngram_suggestions(word, vocab, n=2, top_k=5):\n",
                "    word_ngrams = set(get_char_ngrams(word, n))\n",
                "    scores = []\n",
                "    \n",
                "    for vocab_word in vocab:\n",
                "        v_ngrams = set(get_char_ngrams(vocab_word, n))\n",
                "        score = jaccard_similarity(word_ngrams, v_ngrams)\n",
                "        if score > 0:\n",
                "            scores.append((vocab_word, score))\n",
                "    \n",
                "    return sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
                "\n",
                "# Test N-gram suggestions\n",
                "typo = \"नेपल\" # Correct: नेपाल\n",
                "print(f\"Suggestions for '{typo}':\")\n",
                "print(get_ngram_suggestions(typo, vocabulary.keys()))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Integrated Spell Checker\\n\n",
                "\\n\n",
                "We combine these techniques:\\n\n",
                "1. If word is in vocabulary, it's correct.\n",
                "2. If not, find candidates with high n-gram overlap.\n",
                "3. Rank candidates by Minimum Edit Distance (primary) and Frequency (secondary)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Query: नेपल       | Suggestions: [('नेपाल', 1, 9), ('नेपाली', 2, 9), ('खेल', 2, 1)]\n",
                        "Query: काठमडौ     | Suggestions: [('काठमाडौं', 2, 2), ('काम', 3, 1), ('काठमाडौंमा', 4, 1)]\n",
                        "Query: सरकार      | Suggestions: [('सरकारले', 2, 3), ('सरकारको', 2, 2), ('सुधार', 2, 1)]\n",
                        "Query: बिकास      | Suggestions: [('विकास', 1, 1), ('निकाय', 2, 1), ('इतिहास', 3, 4)]\n"
                    ]
                }
            ],
            "source": [
                "def spell_check(word, vocab_counts, top_k=3):\n",
                "    if word in vocab_counts:\n",
                "        return [(word, 0, vocab_counts[word])] # Exact match\n",
                "    \n",
                "    # 1. Get candidates using bigrams (coarse filter)\n",
                "    candidates = [w for w, score in get_ngram_suggestions(word, vocab_counts.keys(), n=2, top_k=20)]\n",
                "    \n",
                "    # 2. Refine using Edit Distance\n",
                "    refined = []\n",
                "    for cand in candidates:\n",
                "        dist = simple_edit_distance(word, cand)\n",
                "        freq = vocab_counts[cand]\n",
                "        refined.append((cand, dist, freq))\n",
                "    \n",
                "    # Sort by: Distance (asc), then Frequency (desc)\n",
                "    refined.sort(key=lambda x: (x[1], -x[2]))\n",
                "    \n",
                "    return refined[:top_k]\n",
                "\n",
                "# Interactive Test\n",
                "test_words = [\"नेपल\", \"काठमडौ\", \"सरकार\", \"बिकास\"]\n",
                "\n",
                "for w in test_words:\n",
                "    corrections = spell_check(w, vocabulary)\n",
                "    print(f\"Query: {w:<10} | Suggestions: {corrections}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
