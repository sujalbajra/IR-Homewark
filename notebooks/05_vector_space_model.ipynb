{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 05. Vector Space Model (VSM)\n",
                "\n",
                "## Table of Contents\n",
                "1. [Introduction](#introduction)\n",
                "2. [Theory: Vector Space Model](#theory)\n",
                "3. [Document Vectors](#vectors)\n",
                "4. [Cosine Similarity](#cosine)\n",
                "5. [Ranked Retrieval](#ranking)\n",
                "6. [Summary](#summary)\n",
                "\n",
                "---\n",
                "\n",
                "## 1. Introduction <a name=\"introduction\"></a>\n",
                "\n",
                "The **Vector Space Model (VSM)** revolutionized Information Retrieval by introducing:\n",
                "- **Partial matching**: Documents can be somewhat relevant\n",
                "- **Ranking**: Order results by similarity score\n",
                "- **Geometric interpretation**: Documents and queries as vectors\n",
                "\n",
                "### Why VSM?\n",
                "Boolean retrieval is too rigid - documents either match or don't. VSM allows:\n",
                "- ‚úì Ranking documents by relevance\n",
                "- ‚úì Partial matching (query terms need not all appear)\n",
                "- ‚úì Term weighting (some terms more important)\n",
                "\n",
                "---\n",
                "\n",
                "## 2. Theory: Vector Space Model <a name=\"theory\"></a>\n",
                "\n",
                "### Core Idea:\n",
                "Represent documents and queries as **vectors in a high-dimensional space**, where each dimension corresponds to a term.\n",
                "\n",
                "### Example:\n",
                "Vocabulary: {‡§®‡•á‡§™‡§æ‡§≤, ‡§π‡§ø‡§Æ‡§æ‡§≤, ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ, ‡§™‡§∞‡•ç‡§Ø‡§ü‡§ï}\n",
                "\n",
                "```\n",
                "Document 1: \"‡§®‡•á‡§™‡§æ‡§≤ ‡§π‡§ø‡§Æ‡§æ‡§≤‡§ï‡•ã ‡§¶‡•á‡§∂\" ‚Üí [2, 1, 0, 0]\n",
                "Document 2: \"‡§®‡•á‡§™‡§æ‡§≤ ‡§™‡§∞‡•ç‡§Ø‡§ü‡§®\"         ‚Üí [1, 0, 0, 1]\n",
                "Query:      \"‡§®‡•á‡§™‡§æ‡§≤ ‡§π‡§ø‡§Æ‡§æ‡§≤\"         ‚Üí [1, 1, 0, 0]\n",
                "```\n",
                "\n",
                "### Geometric Interpretation:\n",
                "```\n",
                "         ‡§π‡§ø‡§Æ‡§æ‡§≤ ‚Üë\n",
                "               |\n",
                "          Doc1 ‚Ä¢\n",
                "              /|\n",
                "        Query‚Ä¢ | \n",
                "            /  |\n",
                "           /   ‚Ä¢ Doc2\n",
                "          /____________________‚Üí ‡§®‡•á‡§™‡§æ‡§≤\n",
                "```\n",
                "\n",
                "**Similarity = Angle between vectors**\n",
                "- Small angle = High similarity\n",
                "- Large angle = Low similarity\n",
                "\n",
                "### Term Weighting:\n",
                "Not all terms are equally important!\n",
                "\n",
                "1. **Term Frequency (TF)**: How often term appears in document\n",
                "   - More occurrences ‚Üí Higher weight\n",
                "\n",
                "2. **Document Frequency (DF)**: How many documents contain term\n",
                "   - Common terms (high DF) ‚Üí Lower weight\n",
                "   - Rare terms (low DF) ‚Üí Higher weight\n",
                "\n",
                "For now, we'll use simple **term frequency** weighting.\n",
                "\n",
                "---\n",
                "\n",
                "## 3. Document Vectors <a name=\"vectors\"></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úì Loaded 10 documents\n"
                    ]
                }
            ],
            "source": [
                "from pathlib import Path\n",
                "from collections import Counter\n",
                "import math\n",
                "\n",
                "# Load preprocessing utilities\n",
                "DATA_DIR = Path('../data')\n",
                "\n",
                "def load_documents(data_dir):\n",
                "    documents = {}\n",
                "    for file_path in sorted(data_dir.glob('doc*.txt')):\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            documents[file_path.stem] = f.read()\n",
                "    return documents\n",
                "\n",
                "def load_stopwords(file_path):\n",
                "    stopwords = set()\n",
                "    with open(file_path, 'r', encoding='utf-8') as f:\n",
                "        next(f)\n",
                "        for line in f:\n",
                "            stopwords.add(line.strip())\n",
                "    return stopwords\n",
                "\n",
                "def load_stemming_dict(file_path):\n",
                "    stem_dict = {}\n",
                "    with open(file_path, 'r', encoding='utf-8') as f:\n",
                "        next(f)\n",
                "        for line in f:\n",
                "            parts = line.strip().split(',')\n",
                "            if len(parts) == 2:\n",
                "                stem_dict[parts[0]] = parts[1]\n",
                "    return stem_dict\n",
                "\n",
                "def tokenize(text):\n",
                "    tokens = text.split()\n",
                "    cleaned = []\n",
                "    for token in tokens:\n",
                "        token = token.strip('‡•§,.!?;:\"\\'-()[]{}/')\n",
                "        if token and any('\\u0900' <= c <= '\\u097F' for c in token):\n",
                "            cleaned.append(token)\n",
                "    return cleaned\n",
                "\n",
                "def preprocess_text(text, stopwords, stem_dict):\n",
                "    tokens = tokenize(text)\n",
                "    tokens = [t for t in tokens if t not in stopwords]\n",
                "    tokens = [stem_dict.get(t, t) for t in tokens]\n",
                "    return tokens\n",
                "\n",
                "documents = load_documents(DATA_DIR)\n",
                "stopwords = load_stopwords(DATA_DIR / 'nepali_stopwords.csv')\n",
                "stem_dict = load_stemming_dict(DATA_DIR / 'nepali_stemming.csv')\n",
                "\n",
                "preprocessed_docs = {}\n",
                "for doc_id, text in documents.items():\n",
                "    preprocessed_docs[doc_id] = preprocess_text(text, stopwords, stem_dict)\n",
                "\n",
                "print(f\"‚úì Loaded {len(preprocessed_docs)} documents\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úì Built document vectors\n",
                        "  Vocabulary size: 398\n",
                        "  Vector dimensions: 398\n",
                        "  Number of vectors: 10\n"
                    ]
                }
            ],
            "source": [
                "def build_vocabulary(preprocessed_docs):\n",
                "    \"\"\"\n",
                "    Build vocabulary from all documents.\n",
                "    \n",
                "    Returns: sorted list of unique terms\n",
                "    \"\"\"\n",
                "    vocab = set()\n",
                "    for terms in preprocessed_docs.values():\n",
                "        vocab.update(terms)\n",
                "    return sorted(vocab)\n",
                "\n",
                "def build_term_frequency_vectors(preprocessed_docs, vocabulary):\n",
                "    \"\"\"\n",
                "    Build document vectors using term frequency weighting.\n",
                "    \n",
                "    For each document, count how many times each term appears.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    preprocessed_docs : dict\n",
                "        Document ID ‚Üí list of terms\n",
                "    vocabulary : list\n",
                "        Ordered list of all unique terms\n",
                "    \n",
                "    Returns:\n",
                "    --------\n",
                "    dict : Document ID ‚Üí vector (list of term frequencies)\n",
                "    \"\"\"\n",
                "    vectors = {}\n",
                "    \n",
                "    # Create term ‚Üí index mapping\n",
                "    term_to_idx = {term: idx for idx, term in enumerate(vocabulary)}\n",
                "    \n",
                "    for doc_id, terms in preprocessed_docs.items():\n",
                "        # Count term frequencies\n",
                "        term_counts = Counter(terms)\n",
                "        \n",
                "        # Build vector\n",
                "        vector = [0] * len(vocabulary)\n",
                "        for term, count in term_counts.items():\n",
                "            idx = term_to_idx[term]\n",
                "            vector[idx] = count\n",
                "        \n",
                "        vectors[doc_id] = vector\n",
                "    \n",
                "    return vectors\n",
                "\n",
                "# Build vocabulary and vectors\n",
                "vocabulary = build_vocabulary(preprocessed_docs)\n",
                "doc_vectors = build_term_frequency_vectors(preprocessed_docs, vocabulary)\n",
                "\n",
                "print(f\"‚úì Built document vectors\")\n",
                "print(f\"  Vocabulary size: {len(vocabulary)}\")\n",
                "print(f\"  Vector dimensions: {len(vocabulary)}\")\n",
                "print(f\"  Number of vectors: {len(doc_vectors)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üìä Document Vectors (sample):\n",
                        "================================================================================\n",
                        "Doc ID    ‡§®‡•á‡§™‡§æ‡§≤       ‡§π‡§ø‡§Æ‡§æ‡§≤       ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ      ‡§™‡§∞‡•ç‡§Ø‡§ü‡§ï      ‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø   \n",
                        "================================================================================\n",
                        "doc01     6           1           0           0           0           \n",
                        "doc02     4           5           0           4           0           \n",
                        "doc03     5           0           5           0           0           \n",
                        "doc04     5           0           0           0           0           \n",
                        "doc05     6           0           0           0           0           \n",
                        "================================================================================\n",
                        "\n",
                        "Note: Numbers show how many times each term appears in the document\n"
                    ]
                }
            ],
            "source": [
                "# Visualize sample vectors\n",
                "def show_vector_sample(doc_vectors, vocabulary, sample_terms, num_docs=3):\n",
                "    \"\"\"\n",
                "    Display sample document vectors for specific terms.\n",
                "    \"\"\"\n",
                "    # Get indices of sample terms\n",
                "    term_to_idx = {term: idx for idx, term in enumerate(vocabulary)}\n",
                "    sample_indices = [term_to_idx[term] for term in sample_terms if term in term_to_idx]\n",
                "    \n",
                "    print(\"\\nüìä Document Vectors (sample):\")\n",
                "    print(\"=\"*80)\n",
                "    \n",
                "    # Header\n",
                "    header = \"Doc ID    \"\n",
                "    for term in sample_terms:\n",
                "        header += f\"{term[:10]:<12}\"\n",
                "    print(header)\n",
                "    print(\"=\"*80)\n",
                "    \n",
                "    # Show first few documents\n",
                "    for doc_id in sorted(doc_vectors.keys())[:num_docs]:\n",
                "        row = f\"{doc_id:<10}\"\n",
                "        for term in sample_terms:\n",
                "            if term in term_to_idx:\n",
                "                idx = term_to_idx[term]\n",
                "                value = doc_vectors[doc_id][idx]\n",
                "                row += f\"{value:<12}\"\n",
                "            else:\n",
                "                row += f\"{'N/A':<12}\"\n",
                "        print(row)\n",
                "    \n",
                "    print(\"=\"*80)\n",
                "    print(\"\\nNote: Numbers show how many times each term appears in the document\")\n",
                "\n",
                "sample_terms = ['‡§®‡•á‡§™‡§æ‡§≤', '‡§π‡§ø‡§Æ‡§æ‡§≤', '‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ', '‡§™‡§∞‡•ç‡§Ø‡§ü‡§ï', '‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø']\n",
                "show_vector_sample(doc_vectors, vocabulary, sample_terms, num_docs=5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Cosine Similarity <a name=\"cosine\"></a>\n",
                "\n",
                "**Cosine Similarity** measures the angle between two vectors.\n",
                "\n",
                "### Formula:\n",
                "\n",
                "$$\n",
                "\\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{||A|| \\times ||B||}\n",
                "$$\n",
                "\n",
                "Where:\n",
                "- $A \\cdot B$ = Dot product\n",
                "- $||A||$ = Length (magnitude) of vector A\n",
                "\n",
                "### Properties:\n",
                "- Range: [0, 1] for non-negative vectors\n",
                "- 1 = Identical direction (most similar)\n",
                "- 0 = Perpendicular (no similarity)\n",
                "\n",
                "### Why Cosine?\n",
                "- **Length-invariant**: Long and short documents comparable\n",
                "- **Angle-based**: Measures orientation, not magnitude\n",
                "- **Efficient**: Fast to compute with sparse vectors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úì Similarity functions defined\n"
                    ]
                }
            ],
            "source": [
                "def dot_product(vec1, vec2):\n",
                "    \"\"\"\n",
                "    Calculate dot product of two vectors.\n",
                "    \n",
                "    Formula: sum(vec1[i] * vec2[i] for all i)\n",
                "    \"\"\"\n",
                "    return sum(v1 * v2 for v1, v2 in zip(vec1, vec2))\n",
                "\n",
                "def vector_magnitude(vec):\n",
                "    \"\"\"\n",
                "    Calculate magnitude (length) of a vector.\n",
                "    \n",
                "    Formula: sqrt(sum(vec[i]^2 for all i))\n",
                "    \"\"\"\n",
                "    return math.sqrt(sum(v ** 2 for v in vec))\n",
                "\n",
                "def cosine_similarity(vec1, vec2):\n",
                "    \"\"\"\n",
                "    Calculate cosine similarity between two vectors.\n",
                "    \n",
                "    Returns value between 0 and 1:\n",
                "    - 1 = identical\n",
                "    - 0 = no similarity\n",
                "    \"\"\"\n",
                "    dot_prod = dot_product(vec1, vec2)\n",
                "    mag1 = vector_magnitude(vec1)\n",
                "    mag2 = vector_magnitude(vec2)\n",
                "    \n",
                "    # Avoid division by zero\n",
                "    if mag1 == 0 or mag2 == 0:\n",
                "        return 0.0\n",
                "    \n",
                "    return dot_prod / (mag1 * mag2)\n",
                "\n",
                "print(\"‚úì Similarity functions defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üìê Document Similarity Matrix:\n",
                        "======================================================================\n",
                        "      doc01     doc02     doc03     doc07     \n",
                        "======================================================================\n",
                        "doc01 1.0000    0.3028    0.2664    0.1719    \n",
                        "doc02 0.3028    1.0000    0.1950    0.1198    \n",
                        "doc03 0.2664    0.1950    1.0000    0.2258    \n",
                        "doc07 0.1719    0.1198    0.2258    1.0000    \n",
                        "======================================================================\n",
                        "\n",
                        "Interpretation:\n",
                        "  - Diagonal = 1.0 (document compared to itself)\n",
                        "  - Higher values = More similar documents\n"
                    ]
                }
            ],
            "source": [
                "# Test: Compare similarity between documents\n",
                "print(\"\\nüìê Document Similarity Matrix:\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "# Sample documents\n",
                "sample_docs = ['doc01', 'doc02', 'doc03', 'doc07']\n",
                "\n",
                "# Header\n",
                "header = \"      \"\n",
                "for doc in sample_docs:\n",
                "    header += f\"{doc:<10}\"\n",
                "print(header)\n",
                "print(\"=\"*70)\n",
                "\n",
                "# Calculate pairwise similarities\n",
                "for doc1 in sample_docs:\n",
                "    row = f\"{doc1:<6}\"\n",
                "    for doc2 in sample_docs:\n",
                "        sim = cosine_similarity(doc_vectors[doc1], doc_vectors[doc2])\n",
                "        row += f\"{sim:.4f}    \"\n",
                "    print(row)\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"\\nInterpretation:\")\n",
                "print(\"  - Diagonal = 1.0 (document compared to itself)\")\n",
                "print(\"  - Higher values = More similar documents\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Ranked Retrieval <a name=\"ranking\"></a>\n",
                "\n",
                "Now we can rank documents by their similarity to a query!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úì Ranked retrieval functions defined\n"
                    ]
                }
            ],
            "source": [
                "def query_to_vector(query_text, vocabulary, stopwords, stem_dict):\n",
                "    \"\"\"\n",
                "    Convert query text to a vector in the same space as documents.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    query_text : str\n",
                "        Raw query text\n",
                "    vocabulary : list\n",
                "        Ordered vocabulary\n",
                "    \n",
                "    Returns:\n",
                "    --------\n",
                "    list : Query vector\n",
                "    \"\"\"\n",
                "    # Preprocess query\n",
                "    query_terms = preprocess_text(query_text, stopwords, stem_dict)\n",
                "    \n",
                "    # Count term frequencies\n",
                "    term_counts = Counter(query_terms)\n",
                "    \n",
                "    # Build vector\n",
                "    term_to_idx = {term: idx for idx, term in enumerate(vocabulary)}\n",
                "    vector = [0] * len(vocabulary)\n",
                "    \n",
                "    for term, count in term_counts.items():\n",
                "        if term in term_to_idx:\n",
                "            idx = term_to_idx[term]\n",
                "            vector[idx] = count\n",
                "    \n",
                "    return vector\n",
                "\n",
                "def ranked_retrieval(query_text, doc_vectors, vocabulary, stopwords, stem_dict, top_k=5):\n",
                "    \"\"\"\n",
                "    Retrieve and rank documents by similarity to query.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    query_text : str\n",
                "        User's query\n",
                "    doc_vectors : dict\n",
                "        Document vectors\n",
                "    top_k : int\n",
                "        Number of top results to return\n",
                "    \n",
                "    Returns:\n",
                "    --------\n",
                "    list : [(doc_id, similarity_score), ...] sorted by score\n",
                "    \"\"\"\n",
                "    # Convert query to vector\n",
                "    query_vector = query_to_vector(query_text, vocabulary, stopwords, stem_dict)\n",
                "    \n",
                "    # Calculate similarity with each document\n",
                "    scores = []\n",
                "    for doc_id, doc_vector in doc_vectors.items():\n",
                "        score = cosine_similarity(query_vector, doc_vector)\n",
                "        scores.append((doc_id, score))\n",
                "    \n",
                "    # Sort by score (descending)\n",
                "    scores.sort(key=lambda x: x[1], reverse=True)\n",
                "    \n",
                "    # Return top k\n",
                "    return scores[:top_k]\n",
                "\n",
                "print(\"‚úì Ranked retrieval functions defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üîç Query: '‡§®‡•á‡§™‡§æ‡§≤ ‡§π‡§ø‡§Æ‡§æ‡§≤ ‡§™‡§∞‡•ç‡§Ø‡§ü‡§®'\n",
                        "======================================================================\n",
                        "\n",
                        "Rank   Doc ID     Score      Preview\n",
                        "======================================================================\n",
                        "1      doc02      0.7256    ‡§π‡§ø‡§Æ‡§æ‡§≤ ‡§∞ ‡§™‡§∞‡•ç‡§Ø‡§ü‡§®  ‡§®‡•á‡§™‡§æ‡§≤ ‡§π‡§ø‡§Æ‡§æ‡§≤‡§ï‡•ã ‡§¶‡•á‡§∂ ‡§π‡•ã‡•§ ‡§Ø‡§π‡§æ‡§Å ‡§µ‡§ø‡§∂‡•ç‡§µ‡§ï‡•ã...\n",
                        "2      doc01      0.3836    ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§á‡§§‡§ø‡§π‡§æ‡§∏ ‡§∞ ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø  ‡§®‡•á‡§™‡§æ‡§≤ ‡§¶‡§ï‡•ç‡§∑‡§ø‡§£ ‡§è‡§∂‡§ø‡§Ø‡§æ‡§Æ‡§æ ‡§Ö‡§µ...\n",
                        "3      doc09      0.3518    ‡§µ‡§æ‡§§‡§æ‡§µ‡§∞‡§£ ‡§∞ ‡§ú‡§≤‡§µ‡§æ‡§Ø‡•Å  ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§≠‡•å‡§ó‡•ã‡§≤‡§ø‡§ï ‡§µ‡§ø‡§µ‡§ø‡§ß‡§§‡§æ‡§≤‡•á ‡§ó‡§∞‡•ç‡§¶‡§æ ...\n",
                        "4      doc05      0.3303    ‡§≠‡§æ‡§∑‡§æ ‡§∞ ‡§∏‡§æ‡§π‡§ø‡§§‡•ç‡§Ø  ‡§®‡•á‡§™‡§æ‡§≤‡§Æ‡§æ ‡§ß‡•á‡§∞‡•à ‡§≠‡§æ‡§∑‡§æ‡§π‡§∞‡•Ç ‡§¨‡•ã‡§≤‡§ø‡§®‡•ç‡§õ‡§®‡•ç‡•§ ‡§®‡•á...\n",
                        "5      doc04      0.2887    ‡§ï‡•É‡§∑‡§ø ‡§∞ ‡§Ö‡§∞‡•ç‡§•‡§§‡§®‡•ç‡§§‡•ç‡§∞  ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§Ö‡§∞‡•ç‡§•‡§§‡§®‡•ç‡§§‡•ç‡§∞ ‡§Æ‡•Å‡§ñ‡•ç‡§Ø‡§§‡§É ‡§ï‡•É‡§∑‡§ø...\n",
                        "======================================================================\n"
                    ]
                }
            ],
            "source": [
                "# Example Query 1\n",
                "query1 = \"‡§®‡•á‡§™‡§æ‡§≤ ‡§π‡§ø‡§Æ‡§æ‡§≤ ‡§™‡§∞‡•ç‡§Ø‡§ü‡§®\"\n",
                "print(f\"\\nüîç Query: '{query1}'\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "results = ranked_retrieval(query1, doc_vectors, vocabulary, stopwords, stem_dict, top_k=5)\n",
                "\n",
                "print(f\"\\n{'Rank':<6} {'Doc ID':<10} {'Score':<10} {'Preview'}\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "for rank, (doc_id, score) in enumerate(results, 1):\n",
                "    preview = documents[doc_id][:50].replace('\\n', ' ')\n",
                "    print(f\"{rank:<6} {doc_id:<10} {score:.4f}    {preview}...\")\n",
                "\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üîç Query: '‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§™‡•ç‡§∞‡§µ‡§ø‡§ß‡§ø ‡§µ‡§ø‡§∂‡•ç‡§µ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§≤‡§Ø'\n",
                        "======================================================================\n",
                        "\n",
                        "Rank   Doc ID     Score      Title\n",
                        "======================================================================\n",
                        "1      doc03      0.7089    ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§∞ ‡§™‡•ç‡§∞‡§µ‡§ø‡§ß‡§ø\n",
                        "2      doc07      0.0550    ‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø ‡§∏‡•á‡§µ‡§æ\n",
                        "3      doc01      0.0000    ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§á‡§§‡§ø‡§π‡§æ‡§∏ ‡§∞ ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø\n",
                        "4      doc02      0.0000    ‡§π‡§ø‡§Æ‡§æ‡§≤ ‡§∞ ‡§™‡§∞‡•ç‡§Ø‡§ü‡§®\n",
                        "5      doc04      0.0000    ‡§ï‡•É‡§∑‡§ø ‡§∞ ‡§Ö‡§∞‡•ç‡§•‡§§‡§®‡•ç‡§§‡•ç‡§∞\n",
                        "======================================================================\n"
                    ]
                }
            ],
            "source": [
                "# Example Query 2\n",
                "query2 = \"‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§™‡•ç‡§∞‡§µ‡§ø‡§ß‡§ø ‡§µ‡§ø‡§∂‡•ç‡§µ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§≤‡§Ø\"\n",
                "print(f\"\\nüîç Query: '{query2}'\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "results = ranked_retrieval(query2, doc_vectors, vocabulary, stopwords, stem_dict, top_k=5)\n",
                "\n",
                "print(f\"\\n{'Rank':<6} {'Doc ID':<10} {'Score':<10} {'Title'}\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "for rank, (doc_id, score) in enumerate(results, 1):\n",
                "    title = documents[doc_id].split('\\n')[0]\n",
                "    print(f\"{rank:<6} {doc_id:<10} {score:.4f}    {title}\")\n",
                "\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üîç Query: '‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø ‡§Ö‡§∏‡•ç‡§™‡§§‡§æ‡§≤ ‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ'\n",
                        "======================================================================\n",
                        "\n",
                        "Rank   Doc ID     Score      Title\n",
                        "======================================================================\n",
                        "1      doc07      0.6055    ‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø ‡§∏‡•á‡§µ‡§æ\n",
                        "2      doc01      0.0000    ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§á‡§§‡§ø‡§π‡§æ‡§∏ ‡§∞ ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø\n",
                        "3      doc02      0.0000    ‡§π‡§ø‡§Æ‡§æ‡§≤ ‡§∞ ‡§™‡§∞‡•ç‡§Ø‡§ü‡§®\n",
                        "4      doc03      0.0000    ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§∞ ‡§™‡•ç‡§∞‡§µ‡§ø‡§ß‡§ø\n",
                        "5      doc04      0.0000    ‡§ï‡•É‡§∑‡§ø ‡§∞ ‡§Ö‡§∞‡•ç‡§•‡§§‡§®‡•ç‡§§‡•ç‡§∞\n",
                        "======================================================================\n",
                        "\n",
                        "üí° Note: Documents with score 0.0 have no query terms in common\n"
                    ]
                }
            ],
            "source": [
                "# Example Query 3\n",
                "query3 = \"‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø ‡§Ö‡§∏‡•ç‡§™‡§§‡§æ‡§≤ ‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ\"\n",
                "print(f\"\\nüîç Query: '{query3}'\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "results = ranked_retrieval(query3, doc_vectors, vocabulary, stopwords, stem_dict, top_k=5)\n",
                "\n",
                "print(f\"\\n{'Rank':<6} {'Doc ID':<10} {'Score':<10} {'Title'}\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "for rank, (doc_id, score) in enumerate(results, 1):\n",
                "    title = documents[doc_id].split('\\n')[0]\n",
                "    print(f\"{rank:<6} {doc_id:<10} {score:.4f}    {title}\")\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"\\nüí° Note: Documents with score 0.0 have no query terms in common\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Summary <a name=\"summary\"></a>\n",
                "\n",
                "### What We Learned:\n",
                "\n",
                "1. **Vector Space Model**\n",
                "   - Documents and queries as vectors\n",
                "   - Geometric interpretation of relevance\n",
                "   - Partial matching and ranking\n",
                "\n",
                "2. **Term Frequency Vectors**\n",
                "   - Count how often each term appears\n",
                "   - Higher frequency ‚Üí Higher weight\n",
                "   - Vocabulary defines vector dimensions\n",
                "\n",
                "3. **Cosine Similarity**\n",
                "   - Measures angle between vectors\n",
                "   - Range: 0 (no similarity) to 1 (identical)\n",
                "   - Length-invariant (fair for different doc sizes)\n",
                "\n",
                "4. **Ranked Retrieval**\n",
                "   - Order documents by similarity score\n",
                "   - Show most relevant results first\n",
                "   - Much better than Boolean retrieval\n",
                "\n",
                "### Advantages over Boolean:\n",
                "- ‚úì **Ranked results**: Best matches first\n",
                "- ‚úì **Partial matching**: Some query terms can be missing\n",
                "- ‚úì **Graded relevance**: Similarity scores\n",
                "- ‚úì **User-friendly**: Natural language queries\n",
                "\n",
                "### Limitations:\n",
                "- ‚úó **Vocabulary mismatch**: Synonyms not handled\n",
                "- ‚úó **Equal term weights**: All terms treated same\n",
                "- ‚úó **No context**: Word order ignored\n",
                "\n",
                "### Next Steps:\n",
                "In the next notebook (`06_tf_idf_ranking.ipynb`), we will:\n",
                "- Implement TF-IDF weighting\n",
                "- Give rare terms higher importance\n",
                "- Penalize common terms\n",
                "- Improve ranking quality\n",
                "\n",
                "### Research References:\n",
                "- Gerard Salton: Pioneer of VSM (1970s)\n",
                "- Manning et al., \"Introduction to Information Retrieval\", Chapter 6\n",
                "- VSM is the foundation of modern ranking algorithms\n",
                "- Used in Lucene, Elasticsearch, and early Google"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
